{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Horizon River Differential Prediction with Future Rainfall\n",
        "\n",
        "This notebook implements a multi-horizon LSTM model that predicts river differential for the next 10 days (240 hours) using:\n",
        "1. **Historical rainfall data** (for soil moisture context)\n",
        "2. **Historical differential data** (for river state context)\n",
        "3. **Future rainfall forecasts** (key improvement!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "PyTorch version: 2.2.2\n",
            "======================================================================\n",
            "Using device: mps (Apple Silicon GPU)\n",
            "Working directory: /Users/robertds413/Documents/Flag_Predictor\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "import requests\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "\n",
        "# PyTorch imports for LSTM\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "# PyTorch Information\n",
        "print(\"=\"*70)\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Set device for PyTorch - Check for MPS (Apple Silicon), CUDA, then CPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(f\"Using device: cuda (NVIDIA GPU)\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "    print(f\"Using device: mps (Apple Silicon GPU)\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(f\"Using device: cpu\")\n",
        "\n",
        "\n",
        "# Set the working directory\n",
        "os.chdir('/Users/robertds413/Documents/Flag_Predictor')\n",
        "print(f\"Working directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Data loading functions defined\n"
          ]
        }
      ],
      "source": [
        "def process_lock_api_data(data):\n",
        "    \"\"\"Process API data from flood monitoring service\"\"\"\n",
        "    if 'items' not in data or not data['items']:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    temp_df = pd.DataFrame(data['items'])\n",
        "    if 'dateTime' not in temp_df.columns or 'value' not in temp_df.columns:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    temp_df = temp_df[['dateTime', 'value']]\n",
        "    temp_df.rename(columns={'dateTime': 'timestamp', 'value': 'level'}, inplace=True)\n",
        "    temp_df['timestamp'] = pd.to_datetime(temp_df['timestamp'])\n",
        "    df = temp_df.set_index('timestamp')\n",
        "    return df\n",
        "\n",
        "def process_rainfall_api_data(data):\n",
        "    \"\"\"Process rainfall API data from flood monitoring service\"\"\"\n",
        "    if 'items' not in data or not data['items']:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    temp_df = pd.DataFrame(data['items'])\n",
        "    if 'dateTime' not in temp_df.columns or 'value' not in temp_df.columns:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    temp_df = temp_df[['dateTime', 'value']]\n",
        "    temp_df.rename(columns={'dateTime': 'timestamp', 'value': 'rainfall'}, inplace=True)\n",
        "    temp_df['timestamp'] = pd.to_datetime(temp_df['timestamp'])\n",
        "    df = temp_df.set_index('timestamp')\n",
        "    return df\n",
        "\n",
        "def load_flag_model_data(file_path, differential_column):\n",
        "    \"\"\"Load flag model training data from CSV file\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df = df.set_index('timestamp')\n",
        "    df = df[[differential_column]]\n",
        "    df = df.rename(columns={differential_column: 'differential'})\n",
        "    return df\n",
        "\n",
        "print(\"âœ“ Data loading functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load API Data (River Levels & Current Rainfall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching river level data...\n",
            "Fetching rainfall data...\n",
            "\n",
            "âœ“ Combined rainfall API data: (2828, 12)\n",
            "Date range: 2025-12-22 00:00:00+00:00 to 2026-01-20 10:45:00+00:00\n"
          ]
        }
      ],
      "source": [
        "# API URLs\n",
        "kings_mill_downstream_url = 'http://environment.data.gov.uk/flood-monitoring/id/measures/1491TH-level-downstage-i-15_min-mASD/readings?_sorted&_limit=90000'\n",
        "godstow_downstream_url = 'http://environment.data.gov.uk/flood-monitoring/id/measures/1302TH-level-downstage-i-15_min-mASD/readings?_sorted&_limit=90000'\n",
        "osney_upstream_url = 'http://environment.data.gov.uk/flood-monitoring/id/measures/1303TH-level-stage-i-15_min-mASD/readings?_sorted&_limit=90000'\n",
        "osney_downstream_url = 'http://environment.data.gov.uk/flood-monitoring/id/measures/1303TH-level-downstage-i-15_min-mASD/readings?_sorted&_limit=90000'\n",
        "iffley_upstream_url = 'http://environment.data.gov.uk/flood-monitoring/id/measures/1501TH-level-stage-i-15_min-mASD/readings?_sorted&_limit=90000'\n",
        "\n",
        "# Rainfall API URLs\n",
        "rainfall_urls = [\n",
        "    'http://environment.data.gov.uk/flood-monitoring/id/measures/256230TP-rainfall-tipping_bucket_raingauge-t-15_min-mm/readings?_sorted&_limit=90000',\n",
        "    'http://environment.data.gov.uk/flood-monitoring/id/measures/254336TP-rainfall-tipping_bucket_raingauge-t-15_min-mm/readings?_sorted&_limit=90000',\n",
        "    'http://environment.data.gov.uk/flood-monitoring/id/measures/251530TP-rainfall-tipping_bucket_raingauge-t-15_min-mm/readings?_sorted&_limit=90000',\n",
        "    'http://environment.data.gov.uk/flood-monitoring/id/measures/248332TP-rainfall-tipping_bucket_raingauge-t-15_min-mm/readings?_sorted&_limit=90000',\n",
        "    'http://environment.data.gov.uk/flood-monitoring/id/measures/248965TP-rainfall-tipping_bucket_raingauge-t-15_min-mm/readings?_sorted&_limit=90000',\n",
        "    'http://environment.data.gov.uk/flood-monitoring/id/measures/251556TP-rainfall-tipping_bucket_raingauge-t-15_min-mm/readings?_sorted&_limit=90000',\n",
        "    'http://environment.data.gov.uk/flood-monitoring/id/measures/253340TP-rainfall-tipping_bucket_raingauge-t-15_min-mm/readings?_sorted&_limit=90000',\n",
        "    'http://environment.data.gov.uk/flood-monitoring/id/measures/254829TP-rainfall-tipping_bucket_raingauge-t-15_min-mm/readings?_sorted&_limit=90000',\n",
        "    'http://environment.data.gov.uk/flood-monitoring/id/measures/257039TP-rainfall-tipping_bucket_raingauge-t-15_min-mm/readings?_sorted&_limit=90000',\n",
        "    'http://environment.data.gov.uk/flood-monitoring/id/measures/259110TP-rainfall-tipping_bucket_raingauge-t-15_min-mm/readings?_sorted&_limit=90000',\n",
        "    'http://environment.data.gov.uk/flood-monitoring/id/measures/256345TPrainfall-tipping_bucket_raingauge-t-15_min-mm/readings?_sorted&_limit=90000',\n",
        "    'http://environment.data.gov.uk/flood-monitoring/id/measures/249744TP-rainfall-tipping_bucket_raingauge-t-15_min-mm/readings?_sorted&_limit=90000',\n",
        "    'http://environment.data.gov.uk/flood-monitoring/id/measures/253861TP-rainfall-tipping_bucket_raingauge-t-15_min-mm/readings?_sorted&_limit=90000'\n",
        "]\n",
        "\n",
        "location_names = ['Osney', 'Eynsham', 'St', 'Shorncote', 'Rapsgate', 'Stowell', \n",
        "                  'Bourton', 'Chipping', 'Grimsbury', 'Bicester', 'Byfield', 'Swindon', 'Worsham']\n",
        "\n",
        "print(\"Fetching river level data...\")\n",
        "# Get river level data\n",
        "kings_mill_downstream_df = process_lock_api_data(requests.get(kings_mill_downstream_url).json())\n",
        "godstow_downstream_df = process_lock_api_data(requests.get(godstow_downstream_url).json())\n",
        "osney_upstream_df = process_lock_api_data(requests.get(osney_upstream_url).json())\n",
        "osney_downstream_df = process_lock_api_data(requests.get(osney_downstream_url).json())\n",
        "iffley_upstream_df = process_lock_api_data(requests.get(iffley_upstream_url).json())\n",
        "\n",
        "print(\"Fetching rainfall data...\")\n",
        "# Get rainfall data\n",
        "rainfall_api_dfs = {}\n",
        "for url, name in zip(rainfall_urls, location_names):\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    df = process_rainfall_api_data(data)\n",
        "    if not df.empty:\n",
        "        df.rename(columns={'rainfall': name}, inplace=True)\n",
        "        rainfall_api_dfs[name] = df\n",
        "\n",
        "# Combine all rainfall API datasets\n",
        "mega_rainfall_api_df = pd.concat(rainfall_api_dfs.values(), axis=1)\n",
        "print(f\"\\nâœ“ Combined rainfall API data: {mega_rainfall_api_df.shape}\")\n",
        "print(f\"Date range: {mega_rainfall_api_df.index.min()} to {mega_rainfall_api_df.index.max()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Calculate Differentials from API Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ ISIS differential API data: (2828, 1)\n",
            "âœ“ Godstow differential API data: (2828, 1)\n"
          ]
        }
      ],
      "source": [
        "# Calculate isis and godstow differential\n",
        "isis_diff_isis_contrib = 0.71 * (osney_downstream_df['level'] - iffley_upstream_df['level'] - 2.14)\n",
        "isis_diff_cherwell_contrib = 0.29 * (kings_mill_downstream_df['level'] - iffley_upstream_df['level'] - 0.73)\n",
        "isis_differential_api = isis_diff_isis_contrib + isis_diff_cherwell_contrib\n",
        "\n",
        "godstow_differential_api = godstow_downstream_df['level'] - osney_upstream_df['level'] - 1.63\n",
        "\n",
        "# Convert to DataFrames\n",
        "isis_api_diff_df = pd.DataFrame({'differential': isis_differential_api})\n",
        "godstow_api_diff_df = pd.DataFrame({'differential': godstow_differential_api})\n",
        "\n",
        "print(f\"âœ“ ISIS differential API data: {isis_api_diff_df.shape}\")\n",
        "print(f\"âœ“ Godstow differential API data: {godstow_api_diff_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Historical Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading historical rainfall data...\n",
            "\n",
            "âœ“ Combined historical rainfall: (314283, 13)\n",
            "Date range: 2017-02-02 00:00:00 to 2026-01-19 18:15:00\n",
            "\n",
            "Loading historical differential data...\n",
            "âœ“ ISIS historical differential: (216385, 1)\n",
            "âœ“ Godstow historical differential: (196557, 1)\n"
          ]
        }
      ],
      "source": [
        "# Load historical rainfall data\n",
        "rainfall_data_path = './data/rainfall_training_data/'\n",
        "csv_files = glob.glob(os.path.join(rainfall_data_path, '*.csv'))\n",
        "\n",
        "rainfall_dfs = {}\n",
        "print(\"Loading historical rainfall data...\")\n",
        "\n",
        "for csv_file in csv_files:\n",
        "    file_name = os.path.splitext(os.path.basename(csv_file))[0]\n",
        "    \n",
        "    try:\n",
        "        df = pd.read_csv(csv_file, dtype=str, on_bad_lines='warn')\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_name}: {e}\")\n",
        "        continue\n",
        "    \n",
        "    if 'dateTime' not in df.columns or 'value' not in df.columns:\n",
        "        continue\n",
        "    \n",
        "    df = df[['dateTime', 'value']]\n",
        "    df = df.rename(columns={'dateTime': 'timestamp', 'value': f'rainfall_mm_{file_name}'})\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df[f'rainfall_mm_{file_name}'] = pd.to_numeric(df[f'rainfall_mm_{file_name}'], errors='coerce').fillna(0)\n",
        "    df = df.set_index('timestamp')\n",
        "    rainfall_dfs[file_name] = df\n",
        "\n",
        "mega_rainfall_hist_df = pd.concat(rainfall_dfs.values(), axis=1)\n",
        "print(f\"\\nâœ“ Combined historical rainfall: {mega_rainfall_hist_df.shape}\")\n",
        "print(f\"Date range: {mega_rainfall_hist_df.index.min()} to {mega_rainfall_hist_df.index.max()}\")\n",
        "\n",
        "# Load historical differential data\n",
        "print(\"\\nLoading historical differential data...\")\n",
        "godstow_hist_diff_df = load_flag_model_data('data/godstow_flag_model_data.csv', 'jameson_godstow_differential')\n",
        "isis_hist_diff_df = load_flag_model_data('data/isis_flag_model_data.csv', 'jameson_isis_differential')\n",
        "print(f\"âœ“ ISIS historical differential: {isis_hist_diff_df.shape}\")\n",
        "print(f\"âœ“ Godstow historical differential: {godstow_hist_diff_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Merging and Cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Data merging and cleaning function defined\n"
          ]
        }
      ],
      "source": [
        "def merge_and_clean_data(hist_diff_df, mega_rainfall_hist_df, api_diff_df, mega_rainfall_api_df, differential_column):\n",
        "    \"\"\"\n",
        "    Merge flag data with rainfall data and API data, then resample to hourly frequency.\n",
        "    Cleans spurious rainfall values (negatives, extremes >50mm/h, statistical outliers).\n",
        "    \"\"\"\n",
        "    rainfall_hist_df = mega_rainfall_hist_df.copy()\n",
        "    \n",
        "    # Rename rainfall columns for consistency\n",
        "    new_columns = {}\n",
        "    for col in rainfall_hist_df.columns:\n",
        "        if 'mm_' in col and '-' in col:\n",
        "            parts = col.split('mm_')\n",
        "            if len(parts) > 1:\n",
        "                after_mm = parts[1]\n",
        "                if '-' in after_mm:\n",
        "                    new_name = after_mm.split('-')[0]\n",
        "                    new_columns[col] = new_name\n",
        "    if new_columns:\n",
        "        rainfall_hist_df = rainfall_hist_df.rename(columns=new_columns)\n",
        "    \n",
        "    if rainfall_hist_df.index.tz is None:\n",
        "        rainfall_hist_df = rainfall_hist_df.tz_localize('UTC')\n",
        "    \n",
        "    # Join historical data\n",
        "    df = hist_diff_df.join(rainfall_hist_df, how='inner')\n",
        "    \n",
        "    # Merge with API data (API data takes precedence)\n",
        "    df = df.combine_first(mega_rainfall_api_df)\n",
        "    df = df.combine_first(api_diff_df)\n",
        "    \n",
        "    # Resample to hourly\n",
        "    aggregation_rules = {}\n",
        "    for col in df.columns:\n",
        "        if col != differential_column:\n",
        "            aggregation_rules[col] = 'sum'\n",
        "        else:\n",
        "            aggregation_rules[col] = 'mean'\n",
        "    \n",
        "    df_hourly = df.resample('1H').agg(aggregation_rules)\n",
        "    df = df_hourly.copy()\n",
        "    \n",
        "    # Clean data\n",
        "    df = df.dropna(subset=[differential_column])\n",
        "    df.fillna(0, inplace=True)\n",
        "    \n",
        "    # --- NEW: Clean spurious rainfall values ---\n",
        "    rainfall_cols = [col for col in df.columns if col != differential_column]\n",
        "    print(f\"Cleaning rainfall data for {len(rainfall_cols)} stations...\")\n",
        "    \n",
        "    for col in rainfall_cols:\n",
        "        # Remove negative rainfall (sensor errors)\n",
        "        negative_count = (df[col] < 0).sum()\n",
        "        if negative_count > 0:\n",
        "            print(f\"  {col}: Removing {negative_count} negative values\")\n",
        "            df.loc[df[col] < 0, col] = 0\n",
        "        \n",
        "        # Remove physically impossible values (>50mm/hour is extremely rare in UK)\n",
        "        high_count = (df[col] > 50).sum()\n",
        "        if high_count > 0:\n",
        "            print(f\"  {col}: Capping {high_count} values >50mm/hour\")\n",
        "            df.loc[df[col] > 50, col] = 0\n",
        "        \n",
        "    print(\"âœ“ Rainfall cleaning complete\")\n",
        "    \n",
        "    # Remove extreme values in differential\n",
        "    df = df[(df[differential_column] > -0.1) & (df[differential_column] <= 1.5)].copy()\n",
        "    \n",
        "    # Remove spikes: where value changes by >0.5 and then returns to similar value\n",
        "    diff_series = df[differential_column]\n",
        "    changes = diff_series.diff()\n",
        "    next_changes = changes.shift(-1)\n",
        "    \n",
        "    # Detect spikes: large change followed by opposite large change\n",
        "    spike_mask = (np.abs(changes) > 0.5) & (np.abs(next_changes) > 0.5) & (np.sign(changes) != np.sign(next_changes))\n",
        "    \n",
        "    # Replace spikes with interpolated values\n",
        "    if spike_mask.sum() > 0:\n",
        "        print(f\"  Removing {spike_mask.sum()} spike values in differential\")\n",
        "        df.loc[spike_mask, differential_column] = np.nan\n",
        "        df[differential_column] = df[differential_column].interpolate(method='linear')\n",
        "    \n",
        "    # Also remove statistical outliers\n",
        "    window = 6\n",
        "    rolling_mean = df[differential_column].rolling(window=window, center=True).mean()\n",
        "    rolling_std = df[differential_column].rolling(window=window, center=True).std()\n",
        "    outliers = np.abs(df[differential_column] - rolling_mean) > (3 * rolling_std)\n",
        "    \n",
        "    if outliers.sum() > 0:\n",
        "        df.loc[outliers, differential_column] = df[differential_column].rolling(\n",
        "            window=window, center=True\n",
        "        ).median()[outliers]\n",
        "        df[differential_column] = df[differential_column].rolling(\n",
        "            window=3, center=True\n",
        "        ).mean().fillna(df[differential_column])\n",
        "    \n",
        "    return df\n",
        "\n",
        "print(\"âœ“ Data merging and cleaning function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Engineering with Future Rainfall Support\n",
        "\n",
        "**This is the key innovation!** We create features that include future rainfall forecasts.\n",
        "\n",
        "### ðŸ†• IMPROVED FEATURES (to fix underprediction of rapid rises):\n",
        "- **Velocity features**: Rate of change at multiple timescales (1h, 3h, 6h, 12h, 24h)\n",
        "- **Acceleration features**: Second derivative to detect when rise is accelerating\n",
        "- **Momentum indicator**: Trend strength over 12h window\n",
        "- **Rising signals**: Binary flags when differential is actively rising\n",
        "- **Rainfall intensity ratios**: Recent vs historical rainfall comparison\n",
        "- **Rainfall acceleration**: Is rainfall rate increasing?\n",
        "- **Near-term future rainfall**: 6h and 12h ahead (critical for rapid response)\n",
        "- **Flood amplification**: Interaction between recent rainfall and current differential\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Feature engineering function defined\n"
          ]
        }
      ],
      "source": [
        "def create_features_with_future_rainfall(df, future_rainfall_df=None, differential_column='differential'):\n",
        "    \"\"\"\n",
        "    Create features including FUTURE rainfall forecasts for better long-term predictions.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with historical differential and rainfall data (hourly frequency)\n",
        "        future_rainfall_df: DataFrame with future rainfall forecasts (optional)\n",
        "        differential_column: Name of the differential column\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame: DataFrame with all engineered features including future rainfall\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # --- Historical Rainfall Features ---\n",
        "    rainfall_station_cols = [col for col in df.columns if col not in ['differential']]\n",
        "    df[rainfall_station_cols] = df[rainfall_station_cols].fillna(0)\n",
        "    df['catchment_rainfall_total'] = df[rainfall_station_cols].sum(axis=1)\n",
        "    \n",
        "    # ðŸ†• FIX: LOW-FLOW REGIME FEATURES - Detect stable low periods to prevent false upticks\n",
        "    # These help the model recognize when conditions support staying low\n",
        "    \n",
        "    # Hours since significant rainfall (>1mm total)\n",
        "    significant_rain = (df['catchment_rainfall_total'] > 1.0).astype(int)\n",
        "    df['hours_since_rain'] = significant_rain.groupby((significant_rain != significant_rain.shift()).cumsum()).cumcount()\n",
        "    df.loc[significant_rain == 1, 'hours_since_rain'] = 0  # Reset when rain occurs\n",
        "    # Cap at 720 hours (30 days) to avoid extreme values\n",
        "    df['hours_since_rain'] = df['hours_since_rain'].clip(upper=720)\n",
        "    \n",
        "    # Is currently in a \"dry spell\"? (no significant rain for 48+ hours)\n",
        "    df['is_dry_spell'] = (df['hours_since_rain'] > 48).astype(float)\n",
        "    \n",
        "    # Is currently in a \"low flow\" state? (differential < 0.2m)\n",
        "    df['is_low_flow'] = (df[differential_column] < 0.2).astype(float)\n",
        "    \n",
        "    # Combined: dry spell AND low flow - strong stability signal\n",
        "    df['stable_low_regime'] = df['is_dry_spell'] * df['is_low_flow']\n",
        "    \n",
        "    # Recent rainfall trend: is rainfall decreasing? (last 24h vs 24-48h ago)\n",
        "    df['rainfall_trend_48h'] = (\n",
        "        df['catchment_rainfall_total'].rolling(window=24).sum() - \n",
        "        df['catchment_rainfall_total'].rolling(window=24).sum().shift(24)\n",
        "    )\n",
        "    \n",
        "    # Is drainage ongoing? (differential decreasing while no new rain)\n",
        "    df['is_draining'] = (\n",
        "        (df[differential_column].diff(6) < -0.01) &  # Differential decreasing\n",
        "        (df['catchment_rainfall_total'].rolling(window=6).sum() < 1.0)  # No significant recent rain\n",
        "    ).astype(float)\n",
        "    \n",
        "    # Historical rolling features for soil saturation\n",
        "    df['rainfall_rolling_24h'] = df['catchment_rainfall_total'].rolling(window=24).sum()\n",
        "    df['rainfall_rolling_72h'] = df['catchment_rainfall_total'].rolling(window=72).sum()\n",
        "    df['rainfall_rolling_168h'] = df['catchment_rainfall_total'].rolling(window=168).sum()\n",
        "    df['rainfall_rolling_720h'] = df['catchment_rainfall_total'].rolling(window=720).sum()\n",
        "    \n",
        "    # ðŸ†• NEW: Short-term rainfall intensity (recent vs historical)\n",
        "    df['rainfall_rolling_6h'] = df['catchment_rainfall_total'].rolling(window=6).sum()\n",
        "    df['rainfall_rolling_12h'] = df['catchment_rainfall_total'].rolling(window=12).sum()\n",
        "    df['rainfall_intensity_ratio_6_24'] = df['rainfall_rolling_6h'] / (df['rainfall_rolling_24h'] + 0.01)\n",
        "    df['rainfall_intensity_ratio_24_168'] = df['rainfall_rolling_24h'] / (df['rainfall_rolling_168h'] + 0.01)\n",
        "    \n",
        "    # ðŸ†• NEW: Rainfall acceleration (is rainfall increasing?)\n",
        "    df['rainfall_accel_1h'] = df['catchment_rainfall_total'].rolling(window=1).sum().diff(1)\n",
        "    df['rainfall_accel_3h'] = df['catchment_rainfall_total'].rolling(window=3).sum().diff(3)\n",
        "    df['rainfall_accel_6h'] = df['catchment_rainfall_total'].rolling(window=6).sum().diff(6)\n",
        "    df['rainfall_accel_12h'] = df['catchment_rainfall_total'].rolling(window=12).sum().diff(12)\n",
        "    df['rainfall_accel_24h'] = df['catchment_rainfall_total'].rolling(window=24).sum().diff(24)\n",
        "    df['rainfall_accel_48h'] = df['catchment_rainfall_total'].rolling(window=48).sum().diff(48)\n",
        "    \n",
        "    # --- NEW: Future Rainfall Features ---\n",
        "    if future_rainfall_df is not None:\n",
        "        # Ensure timezone alignment\n",
        "        if future_rainfall_df.index.tz is None and df.index.tz is not None:\n",
        "            future_rainfall_df = future_rainfall_df.tz_localize(df.index.tz)\n",
        "        elif future_rainfall_df.index.tz is not None and df.index.tz is None:\n",
        "            df.index = df.index.tz_localize(future_rainfall_df.index.tz)\n",
        "        \n",
        "        # Align future rainfall with df index\n",
        "        future_rainfall_aligned = future_rainfall_df.reindex(df.index)\n",
        "        \n",
        "        # Create forward-looking rainfall totals for different horizons\n",
        "        catchment_future = future_rainfall_aligned.sum(axis=1)\n",
        "        \n",
        "        # ðŸ†• IMPROVED: Catchment response lag - rain takes time to affect river levels\n",
        "        # Typical catchment lag is 12-24 hours for this river system\n",
        "        CATCHMENT_LAG = 18  # hours - rain at time T affects river at T+18h\n",
        "        \n",
        "        # Rolling FORWARD windows WITH CATCHMENT LAG\n",
        "        # These represent rainfall that will have had time to reach the river\n",
        "        # For horizon H, we look at rainfall from (H - CATCHMENT_LAG) to H\n",
        "        df['rainfall_future_24h'] = catchment_future.rolling(window=24).sum().shift(-24 - CATCHMENT_LAG)\n",
        "        df['rainfall_future_48h'] = catchment_future.rolling(window=48).sum().shift(-48 - CATCHMENT_LAG)\n",
        "        df['rainfall_future_72h'] = catchment_future.rolling(window=72).sum().shift(-72 - CATCHMENT_LAG)\n",
        "        df['rainfall_future_120h'] = catchment_future.rolling(window=120).sum().shift(-120)\n",
        "        df['rainfall_future_240h'] = catchment_future.rolling(window=240).sum().shift(-240)\n",
        "        \n",
        "        # ðŸ†• IMPROVED: Near-term future rainfall WITH LAG\n",
        "        # Rain in next 6-12h won't affect river yet - need to account for catchment travel time\n",
        "        df['rainfall_future_6h'] = catchment_future.rolling(window=6).sum().shift(-6 - CATCHMENT_LAG)\n",
        "        df['rainfall_future_12h'] = catchment_future.rolling(window=12).sum().shift(-12 - CATCHMENT_LAG)\n",
        "        \n",
        "        # ðŸ†• IMPROVED: Horizon-aligned future rainfall WITH LAG\n",
        "        # For each prediction horizon, show rainfall that will have arrived by then\n",
        "        # For 0-24h: every 2 hours (with lag, so rain that fell CATCHMENT_LAG hours before)\n",
        "        for h in [2, 4, 8, 10, 14, 16, 18, 20, 22]:\n",
        "            # For short horizons < CATCHMENT_LAG, use historical/recent rain instead\n",
        "            effective_shift = max(h, CATCHMENT_LAG) \n",
        "            df[f'rainfall_future_{h}h'] = catchment_future.rolling(window=h).sum().shift(-effective_shift)\n",
        "        # For 24-48h: every 6 hours\n",
        "        for h in [30, 36, 42]:\n",
        "            df[f'rainfall_future_{h}h'] = catchment_future.rolling(window=h).sum().shift(-h)\n",
        "        \n",
        "        # ðŸ†• FIX: FUTURE DRY FEATURES - Detect when future has no significant rain\n",
        "        # These help prevent false upticks when no rain is expected\n",
        "        \n",
        "        # Is future dry? (less than 5mm total over entire forecast)\n",
        "        df['future_is_dry_240h'] = (df['rainfall_future_240h'] < 5.0).astype(float)\n",
        "        df['future_is_dry_120h'] = (df['rainfall_future_120h'] < 3.0).astype(float)\n",
        "        df['future_is_dry_72h'] = (df['rainfall_future_72h'] < 2.0).astype(float)\n",
        "        df['future_is_dry_48h'] = (df['rainfall_future_48h'] < 1.0).astype(float)\n",
        "        \n",
        "        # Stability signal: currently low + future dry = should stay low\n",
        "        df['expect_stable_low'] = df['stable_low_regime'] * df['future_is_dry_240h']\n",
        "        \n",
        "        # Future rainfall intensity (mm per hour on average)\n",
        "        df['future_rainfall_intensity_240h'] = df['rainfall_future_240h'] / 240\n",
        "        df['future_rainfall_intensity_120h'] = df['rainfall_future_120h'] / 120\n",
        "        df['future_rainfall_intensity_72h'] = df['rainfall_future_72h'] / 72\n",
        "    \n",
        "    # --- River Differential Features (historical lags) ---\n",
        "    for i in [1, 2, 3, 6, 12, 24, 48]:\n",
        "        df[f'differential_lag_{i}h'] = df[differential_column].shift(i)\n",
        "    \n",
        "    df['differential_rolling_mean_6h'] = df[differential_column].rolling(window=6).mean()\n",
        "    df['differential_rolling_std_6h'] = df[differential_column].rolling(window=6).std()\n",
        "    df['differential_rolling_mean_24h'] = df[differential_column].rolling(window=24).mean()\n",
        "    df['differential_rolling_std_24h'] = df[differential_column].rolling(window=24).std()\n",
        "    df['differential_roc_6h'] = df[differential_column].diff(periods=6)\n",
        "    df['differential_ewma_6h'] = df[differential_column].ewm(span=6, adjust=False).mean()\n",
        "    \n",
        "    # ðŸ†• NEW: Rate of Change Features (velocity) - CRITICAL for detecting rising limbs\n",
        "    for i in [1, 3, 6, 12, 24]:\n",
        "        df[f'differential_velocity_{i}h'] = df[differential_column].diff(periods=i) / i\n",
        "    \n",
        "    # ðŸ†• NEW: Acceleration Features (second derivative) - Detect when rise is accelerating\n",
        "    for i in [3, 6, 12, 24]:\n",
        "        velocity = df[differential_column].diff(periods=i)\n",
        "        df[f'differential_acceleration_{i}h'] = velocity.diff(periods=i) / i\n",
        "    \n",
        "    # ðŸ†• NEW: Momentum indicator (trend strength)\n",
        "    df['differential_momentum_12h'] = df[differential_column].rolling(window=12).apply(\n",
        "        lambda x: (x.iloc[-1] - x.iloc[0]) / (x.std() + 0.001) if len(x) > 0 else 0,\n",
        "        raw=False\n",
        "    )\n",
        "    \n",
        "    # ðŸ†• NEW: Is differential rising? (binary signal)\n",
        "    df['is_rising_6h'] = (df['differential_velocity_6h'] > 0.01).astype(float)\n",
        "    df['is_rising_24h'] = (df['differential_velocity_24h'] > 0.01).astype(float)\n",
        "    \n",
        "    # Interaction features\n",
        "    df['rainfall_last_hour'] = df['catchment_rainfall_total']\n",
        "    df['rainfall_interaction_720h'] = df['rainfall_last_hour'] * df['rainfall_rolling_720h']\n",
        "    \n",
        "    # ðŸ†• NEW: Critical interaction - recent rainfall * current differential (flood amplification)\n",
        "    df['flood_amplification_24h'] = df['rainfall_rolling_24h'] * df[differential_column]\n",
        "    df['flood_amplification_72h'] = df['rainfall_rolling_72h'] * df[differential_column]\n",
        "    df['flood_amplification_168h'] = df['rainfall_rolling_168h'] * df[differential_column]\n",
        "    \n",
        "    # ðŸ†• IMPROVED: Future rainfall * current differential (predictive flood risk)\n",
        "    # These now incorporate catchment lag from the rainfall features\n",
        "    df['future_flood_risk_24h'] = df['rainfall_future_24h'] * df[differential_column]\n",
        "    df['future_flood_risk_48h'] = df['rainfall_future_48h'] * df[differential_column]\n",
        "    df['future_flood_risk_72h'] = df['rainfall_future_72h'] * df[differential_column]\n",
        "    \n",
        "    # ðŸ†• NEW: Lagged rainfall impact - emphasize recent rain that's still in transit\n",
        "    # Rain from 12-24h ago is currently flowing through catchment\n",
        "    df['rainfall_in_transit_12_24h'] = df['catchment_rainfall_total'].rolling(window=12).sum().shift(12)\n",
        "    df['rainfall_in_transit_6_18h'] = df['catchment_rainfall_total'].rolling(window=12).sum().shift(6)\n",
        "    \n",
        "    # --- ðŸ†• IMPROVED: Multi-day river state context (helps shape and timing) ---\n",
        "    # 3-day (72h) behaviour\n",
        "    df['differential_rolling_mean_72h'] = df[differential_column].rolling(window=72).mean()\n",
        "    df['differential_rolling_max_72h'] = df[differential_column].rolling(window=72).max()\n",
        "    df['differential_rolling_min_72h'] = df[differential_column].rolling(window=72).min()\n",
        "    df['differential_range_72h'] = (\n",
        "        df['differential_rolling_max_72h'] - df['differential_rolling_min_72h']\n",
        "    )\n",
        "    \n",
        "    # 7-day (168h) behaviour\n",
        "    df['differential_rolling_mean_168h'] = df[differential_column].rolling(window=168).mean()\n",
        "    df['differential_rolling_max_168h'] = df[differential_column].rolling(window=168).max()\n",
        "    df['differential_rolling_min_168h'] = df[differential_column].rolling(window=168).min()\n",
        "    df['differential_range_168h'] = (\n",
        "        df['differential_rolling_max_168h'] - df['differential_rolling_min_168h']\n",
        "    )\n",
        "    \n",
        "    # Antecedent wetness index: blends short, medium, long rainfall\n",
        "    df['antecedent_wetness'] = (\n",
        "        0.5 * df['rainfall_rolling_24h']\n",
        "        + 0.3 * df['rainfall_rolling_72h']\n",
        "        + 0.2 * df['rainfall_rolling_168h']\n",
        "    )\n",
        "    \n",
        "    # Fast vs slow runoff proxies: exponential rainfall memory\n",
        "    for span, name in [(6, 'fast'), (24, 'medium'), (72, 'slow')]:\n",
        "        df[f'catchment_rainfall_exp_{name}'] = (\n",
        "            df['catchment_rainfall_total']\n",
        "            .ewm(span=span, adjust=False)\n",
        "            .mean()\n",
        "        )\n",
        "    \n",
        "    # --- Cyclical Features ---\n",
        "    df['day_of_year'] = df.index.dayofyear\n",
        "    df['day_of_year_sin'] = np.sin(2 * np.pi * df['day_of_year']/365.25)\n",
        "    df['day_of_year_cos'] = np.cos(2 * np.pi * df['day_of_year']/365.25)\n",
        "    \n",
        "    return df\n",
        "\n",
        "print(\"âœ“ Feature engineering function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Multi-Horizon LSTM Model\n",
        "\n",
        "The model predicts all time horizons simultaneously (24h, 48h, ..., 240h) from a single input sequence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Multi-Horizon LSTM Model defined\n"
          ]
        }
      ],
      "source": [
        "class MultiHorizonLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes=[128, 64], dropout_rate=0.2, n_horizons=10):\n",
        "        \"\"\"\n",
        "        LSTM that predicts multiple future time horizons simultaneously.\n",
        "        \n",
        "        Args:\n",
        "            input_size: Number of input features\n",
        "            hidden_sizes: List of hidden layer sizes\n",
        "            dropout_rate: Dropout rate\n",
        "            n_horizons: Number of future horizons to predict\n",
        "        \"\"\"\n",
        "        super(MultiHorizonLSTMModel, self).__init__()\n",
        "        \n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.num_layers = len(hidden_sizes)\n",
        "        self.n_horizons = n_horizons\n",
        "        \n",
        "        # Create LSTM layers\n",
        "        self.lstm_layers = nn.ModuleList()\n",
        "        self.dropout_layers = nn.ModuleList()\n",
        "        \n",
        "        for i, hidden_size in enumerate(hidden_sizes):\n",
        "            input_dim = input_size if i == 0 else hidden_sizes[i-1]\n",
        "            self.lstm_layers.append(\n",
        "                nn.LSTM(input_dim, hidden_size, batch_first=True)\n",
        "            )\n",
        "            self.dropout_layers.append(nn.Dropout(dropout_rate))\n",
        "        \n",
        "        # Multiple output heads for different time horizons\n",
        "        self.fc_layers = nn.ModuleList([\n",
        "            nn.Linear(hidden_sizes[-1], 1) for _ in range(n_horizons)\n",
        "        ])\n",
        "    \n",
        "    def forward(self, x, debug=False):\n",
        "        # x shape: (batch_size, sequence_length, input_size)\n",
        "        \n",
        "        if debug:\n",
        "            print(f\"[FORWARD] Input shape: {x.shape}\")\n",
        "            import sys\n",
        "            sys.stdout.flush()\n",
        "        \n",
        "        for i, (lstm, dropout) in enumerate(zip(self.lstm_layers, self.dropout_layers)):\n",
        "            if debug:\n",
        "                print(f\"[FORWARD] Running LSTM layer {i+1}/{len(self.lstm_layers)}...\")\n",
        "                sys.stdout.flush()\n",
        "            x, (h_n, c_n) = lstm(x)\n",
        "            if debug:\n",
        "                print(f\"[FORWARD] âœ“ LSTM layer {i+1} done, shape: {x.shape}\")\n",
        "                sys.stdout.flush()\n",
        "            x = dropout(x)\n",
        "        \n",
        "        if debug:\n",
        "            print(f\"[FORWARD] Taking last timestep...\")\n",
        "            sys.stdout.flush()\n",
        "        # Take the output from the last time step\n",
        "        x = x[:, -1, :]\n",
        "        if debug:\n",
        "            print(f\"[FORWARD] âœ“ Last timestep extracted, shape: {x.shape}\")\n",
        "            sys.stdout.flush()\n",
        "        \n",
        "        # Predict multiple horizons\n",
        "        if debug:\n",
        "            print(f\"[FORWARD] Running {len(self.fc_layers)} output heads...\")\n",
        "            sys.stdout.flush()\n",
        "        predictions = []\n",
        "        for fc in self.fc_layers:\n",
        "            predictions.append(fc(x))\n",
        "        \n",
        "        if debug:\n",
        "            print(f\"[FORWARD] âœ“ All output heads done\")\n",
        "            sys.stdout.flush()\n",
        "        \n",
        "        # Stack predictions: (batch_size, n_horizons)\n",
        "        predictions = torch.cat(predictions, dim=1)\n",
        "        \n",
        "        if debug:\n",
        "            print(f\"[FORWARD] âœ“ Predictions stacked, final shape: {predictions.shape}\")\n",
        "            sys.stdout.flush()\n",
        "        \n",
        "        return predictions\n",
        "\n",
        "print(\"âœ“ Multi-Horizon LSTM Model defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Functions for Multi-Horizon Model\n",
        "\n",
        "### ðŸ†• IMPROVED TRAINING (weighted loss function):\n",
        "- **3x weight** for high-flow events (>0.3m) - forces model to pay attention to floods\n",
        "- **1.5x additional weight** for rising events (>5cm increase) - prioritizes rapid rises\n",
        "- This addresses the systematic underprediction of flood peaks!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Sequence creation functions defined\n"
          ]
        }
      ],
      "source": [
        "def create_sequences_lstm_multihorizon(X, y_multi, sequence_length=24):\n",
        "    \"\"\"\n",
        "    Create sequences for multi-horizon LSTM training - OPTIMIZED VERSION.\n",
        "    \n",
        "    Args:\n",
        "        X: Feature DataFrame\n",
        "        y_multi: Target DataFrame with multiple horizons (columns)\n",
        "        sequence_length: Number of time steps to look back\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (X_sequences, y_sequences, valid_indices)\n",
        "    \"\"\"\n",
        "    # Convert to numpy arrays ONCE (much faster than repeated .iloc calls)\n",
        "    X_values = X.values\n",
        "    y_values = y_multi.values\n",
        "    \n",
        "    # Pre-allocate arrays (much faster than appending to lists)\n",
        "    n_samples = len(X) - sequence_length\n",
        "    n_features = X_values.shape[1]\n",
        "    n_horizons = y_values.shape[1]\n",
        "    \n",
        "    X_sequences = np.zeros((n_samples, sequence_length, n_features), dtype=np.float32)\n",
        "    y_sequences = np.zeros((n_samples, n_horizons), dtype=np.float32)\n",
        "    valid_indices = []\n",
        "    \n",
        "    # Vectorized sequence creation\n",
        "    for i in range(n_samples):\n",
        "        X_sequences[i] = X_values[i:i+sequence_length]\n",
        "        y_sequences[i] = y_values[i+sequence_length]\n",
        "        valid_indices.append(X.index[i+sequence_length])\n",
        "    \n",
        "    return X_sequences, y_sequences, valid_indices\n",
        "\n",
        "def create_target_and_X_y_multihorizon(df_featureless, future_rainfall_df, differential_column='differential', \n",
        "                                       horizons=[24, 48, 72, 96, 120, 144, 168, 192, 216, 240]):\n",
        "    \"\"\"\n",
        "    Create multi-horizon targets and feature set.\n",
        "    \n",
        "    Args:\n",
        "        df_featureless: DataFrame with differential and rainfall (no features yet)\n",
        "        future_rainfall_df: DataFrame with future rainfall forecasts\n",
        "        differential_column: Name of the differential column to predict\n",
        "        horizons: List of hours ahead to predict\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (X, y_multi, mask, horizons)\n",
        "    \"\"\"\n",
        "    print(f\"\\nCreating features with future rainfall...\")\n",
        "    \n",
        "    # Create features WITH future rainfall\n",
        "    df_with_features = create_features_with_future_rainfall(df_featureless, future_rainfall_df, differential_column)\n",
        "    \n",
        "    print(f\"Creating targets for {len(horizons)} horizons...\")\n",
        "    \n",
        "    # Create multiple target variables (one for each horizon)\n",
        "    targets = []\n",
        "    for horizon in horizons:\n",
        "        target_col = f'target_{horizon}h'\n",
        "        df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
        "        targets.append(target_col)\n",
        "    \n",
        "    # Define features (exclude targets and differential)\n",
        "    exclude_cols = targets + [differential_column] + [col for col in df_with_features.columns if 'target_' in col]\n",
        "    features = [col for col in df_with_features.columns if col not in exclude_cols]\n",
        "    \n",
        "    X = df_with_features[features]\n",
        "    y_multi = df_with_features[targets]\n",
        "    \n",
        "    # Remove rows where ANY target is NaN\n",
        "    mask = ~y_multi.isna().any(axis=1)\n",
        "    X = X[mask]\n",
        "    y_multi = y_multi[mask]\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Multi-Horizon Model Setup:\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Number of features: {len(features)}\")\n",
        "    print(f\"Number of horizons: {len(horizons)}\")\n",
        "    print(f\"Horizons: {horizons}\")\n",
        "    print(f\"Training samples: {len(X)}\")\n",
        "    print(f\"Date range: {X.index.min()} to {X.index.max()}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    return X, y_multi, mask, horizons\n",
        "\n",
        "print(\"âœ“ Sequence creation functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Training function defined\n"
          ]
        }
      ],
      "source": [
        "def train_multihorizon_model(X, y_multi, horizons, sequence_length=24, epochs=50, batch_size=32, \n",
        "                            validation_split=0.2, learning_rate=0.0001, patience=10, max_grad_norm=1.0,\n",
        "                            hidden_sizes=[192, 128, 64], dropout_rate=0.3):\n",
        "    \"\"\"\n",
        "    Train multi-horizon PyTorch LSTM model.\n",
        "    \n",
        "    Args:\n",
        "        hidden_sizes: List of hidden layer sizes for the LSTM stack (default: [192, 128, 64])\n",
        "        dropout_rate: Dropout rate between layers (default: 0.3)\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (model, scaler, history, sequence_length, horizons)\n",
        "    \"\"\"\n",
        "    # Scale the features\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = pd.DataFrame(\n",
        "        scaler.fit_transform(X),\n",
        "        columns=X.columns,\n",
        "        index=X.index\n",
        "    )\n",
        "    \n",
        "    # Create sequences\n",
        "    X_seq, y_seq, _ = create_sequences_lstm_multihorizon(X_scaled, y_multi, sequence_length)\n",
        "    \n",
        "    print(f\"\\nSequence shape: {X_seq.shape}\")\n",
        "    print(f\"Target shape: {y_seq.shape}\")\n",
        "    \n",
        "    # Split into train and validation (time series split)\n",
        "    n_train = int(len(X_seq) * (1 - validation_split))\n",
        "    X_train, X_val = X_seq[:n_train], X_seq[n_train:]\n",
        "    y_train, y_val = y_seq[:n_train], y_seq[n_train:]\n",
        "    \n",
        "    # Convert to PyTorch tensors - KEEP ON CPU for DataLoader\n",
        "    # This fixes MPS hanging issue with DataLoader + shuffle\n",
        "    X_train_tensor = torch.FloatTensor(X_train)\n",
        "    y_train_tensor = torch.FloatTensor(y_train)\n",
        "    X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
        "    y_val_tensor = torch.FloatTensor(y_val).to(device)\n",
        "    \n",
        "    # Create data loaders - data stays on CPU during loading\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
        "                             num_workers=0, pin_memory=False)\n",
        "    \n",
        "    # Initialize model with configurable architecture\n",
        "    n_features = X_seq.shape[2]\n",
        "    n_horizons = len(horizons)\n",
        "    model = MultiHorizonLSTMModel(input_size=n_features, hidden_sizes=hidden_sizes, \n",
        "                                  dropout_rate=dropout_rate, n_horizons=n_horizons)\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # ðŸ†• Horizon importance: emphasise near-term horizons in the loss\n",
        "    # 0-24h weighted 2.0x, 24-72h weighted 1.5x, 72-240h weighted 1.0x\n",
        "    horizon_importance = []\n",
        "    for h in horizons:\n",
        "        if h <= 24:\n",
        "            horizon_importance.append(2.0)   # strongest weight for short term\n",
        "        elif h <= 72:\n",
        "            horizon_importance.append(1.5)   # medium weight\n",
        "        else:\n",
        "            horizon_importance.append(1.0)   # baseline\n",
        "    horizon_importance = torch.tensor(horizon_importance, device=device).view(1, -1)\n",
        "    print(f\"Horizon importance weights: {horizon_importance.squeeze().tolist()}\")\n",
        "    \n",
        "    print(\"\\nModel Architecture:\")\n",
        "    print(model)\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Training Configuration:\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Training samples: {len(X_train)}\")\n",
        "    print(f\"Validation samples: {len(X_val)}\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "    print(f\"Epochs: {epochs}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    import sys\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    print(\"\\n[DEBUG] Creating loss criterion...\")\n",
        "    sys.stdout.flush()\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    print(\"[DEBUG] âœ“ Loss criterion created\")\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    print(\"[DEBUG] Creating optimizer...\")\n",
        "    sys.stdout.flush()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    print(\"[DEBUG] âœ“ Optimizer created\")\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    print(\"[DEBUG] Creating learning rate scheduler...\")\n",
        "    sys.stdout.flush()\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5, verbose=False\n",
        "    )\n",
        "    print(\"[DEBUG] âœ“ Scheduler created\")\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    print(\"[DEBUG] Initializing training variables...\")\n",
        "    sys.stdout.flush()\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'train_mae': [],\n",
        "        'val_mae': []\n",
        "    }\n",
        "    \n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "    print(\"[DEBUG] âœ“ Variables initialized\")\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STARTING TRAINING\")\n",
        "    print(\"=\"*70)\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    # Test first batch to catch errors early\n",
        "    print(\"\\n[DEBUG] Getting first batch from DataLoader...\")\n",
        "    sys.stdout.flush()\n",
        "    test_batch = next(iter(train_loader))\n",
        "    print(f\"[DEBUG] âœ“ Got first batch\")\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    print(f\"[DEBUG] Moving batch to device ({device})...\")\n",
        "    sys.stdout.flush()\n",
        "    test_X, test_y = test_batch[0].to(device), test_batch[1].to(device)\n",
        "    print(f\"[DEBUG] âœ“ Batch moved to {device}\")\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    print(\"[DEBUG] Setting model to training mode...\")\n",
        "    sys.stdout.flush()\n",
        "    model.train()\n",
        "    print(\"[DEBUG] âœ“ Model in training mode\")\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    print(\"[DEBUG] Running forward pass...\")\n",
        "    sys.stdout.flush()\n",
        "    test_output = model(test_X)\n",
        "    print(f\"[DEBUG] âœ“ Forward pass complete, output shape: {test_output.shape}\")\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    print(\"[DEBUG] Computing loss...\")\n",
        "    sys.stdout.flush()\n",
        "    test_loss = criterion(test_output, test_y)\n",
        "    print(f\"[DEBUG] âœ“ Loss computed: {test_loss.item():.6f}\")\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"âœ“ FIRST BATCH TEST PASSED! Loss: {test_loss.item():.6f}\")\n",
        "    print(\"=\"*70)\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    import time\n",
        "    print(\"\\n[DEBUG] Starting epoch loop...\")\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"EPOCH {epoch+1}/{epochs}\")\n",
        "        print(f\"{'='*70}\")\n",
        "        sys.stdout.flush()\n",
        "        epoch_start_time = time.time()\n",
        "        \n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        train_maes = []\n",
        "        \n",
        "        # Progress bar\n",
        "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=True)\n",
        "        \n",
        "        for batch_X, batch_y in pbar:\n",
        "            # Move batch to device (MPS/CUDA/CPU)\n",
        "            batch_X = batch_X.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(batch_X)\n",
        "            \n",
        "            # ðŸ†• NEW: Weighted loss - penalize errors during high-flow events more\n",
        "            # Weight high-flow predictions (>0.3m) 3x more than low-flow\n",
        "            weights = torch.where(batch_y > 0.3, 3.0, 1.0)\n",
        "            # Also weight rising events (where any horizon shows increase)\n",
        "            rising_mask = (batch_y > batch_y[:, 0:1] + 0.05)  # If rising >5cm\n",
        "            weights = torch.where(rising_mask, weights * 1.5, weights)\n",
        "            \n",
        "            # ðŸ†• FIX: Reduce weight for low-flow predictions at far horizons\n",
        "            # This prevents the model from being penalized for staying near current low value\n",
        "            low_flow_mask = (batch_y[:, 0:1] < 0.2)  # Currently low\n",
        "            stable_mask = (batch_y[:, -1:] < 0.25)  # Stays low at end\n",
        "            low_stable_mask = low_flow_mask & stable_mask\n",
        "            # For low-stable cases, don't up-weight far horizons\n",
        "            weights = torch.where(\n",
        "                low_stable_mask.expand_as(weights),\n",
        "                torch.clamp(weights, max=1.5),  # Cap weights for stable low periods\n",
        "                weights\n",
        "            )\n",
        "            \n",
        "            # Calculate weighted MSE loss with horizon importance\n",
        "            mse_loss = (outputs - batch_y) ** 2\n",
        "            # Combine event/level weights + horizon importance\n",
        "            weighted_loss = (mse_loss * weights * horizon_importance).mean()\n",
        "            \n",
        "            # ðŸ†• FIX: Early rising event detection loss - penalize missing early signs of rises\n",
        "            # This addresses the lag issue by forcing the model to detect rising events earlier\n",
        "            early_rise_loss = 0.0\n",
        "            if outputs.shape[1] >= 2:\n",
        "                # Check if actual is rising (comparing first horizon to later horizons)\n",
        "                actual_rising = (batch_y[:, -1] > batch_y[:, 0] + 0.02)  # Rising >2cm over forecast\n",
        "                # Check if prediction is rising\n",
        "                pred_rising = (outputs[:, -1] > outputs[:, 0] + 0.02)\n",
        "                \n",
        "                # Penalize when actual is rising but prediction isn't (missed early detection)\n",
        "                missed_rise_mask = actual_rising & ~pred_rising\n",
        "                if missed_rise_mask.any():\n",
        "                    # Calculate how much the prediction missed the early rise\n",
        "                    actual_rise_magnitude = batch_y[:, -1] - batch_y[:, 0]\n",
        "                    pred_rise_magnitude = outputs[:, -1] - outputs[:, 0]\n",
        "                    rise_error = torch.clamp(actual_rise_magnitude - pred_rise_magnitude, min=0)\n",
        "                    early_rise_loss = (rise_error * missed_rise_mask.float()).mean()\n",
        "            \n",
        "            # (Old anchor_loss removed - replaced by extended_anchor_loss below)\n",
        "            \n",
        "            # ðŸ†• NEW: Continuity loss - first prediction must be close to current value\n",
        "            # This ensures smooth transition from observed to predicted\n",
        "            continuity_loss = torch.mean((outputs[:, 0] - batch_y[:, 0]) ** 2)\n",
        "            \n",
        "            # ðŸ†• FIX: PERSISTENCE LOSS - Penalize predicting upticks when actual stays low\n",
        "            # This is the KEY fix for the mean-reversion bias\n",
        "            persistence_loss = 0.0\n",
        "            current_val = batch_y[:, 0]  # Current differential value\n",
        "            \n",
        "            # Identify stable-low cases: currently low AND ends low\n",
        "            is_stable_low = (current_val < 0.2) & (batch_y[:, -1] < 0.25)\n",
        "            \n",
        "            if is_stable_low.any():\n",
        "                # For stable-low cases, penalize predictions that diverge upward\n",
        "                for i in range(outputs.shape[1]):\n",
        "                    # How much does prediction rise above current?\n",
        "                    pred_rise = torch.clamp(outputs[:, i] - current_val - 0.02, min=0)  # 2cm tolerance\n",
        "                    # Only apply to stable-low cases\n",
        "                    persistence_loss += (pred_rise * is_stable_low.float()).mean()\n",
        "                persistence_loss = persistence_loss / outputs.shape[1]\n",
        "            \n",
        "            # ðŸ†• FIX: EXTENDED ANCHOR LOSS - Apply to ALL horizons, not just first 6\n",
        "            # For low-flow cases, anchor predictions more strongly to current value\n",
        "            extended_anchor_loss = 0.0\n",
        "            for i in range(outputs.shape[1]):\n",
        "                # Weight decays with horizon, but doesn't disappear entirely\n",
        "                weight = 0.3 / (1 + i * 0.1)  # Slower decay than before\n",
        "                # Stronger anchoring for low-flow cases\n",
        "                anchor_strength = torch.where(current_val < 0.2, 2.0, 1.0)\n",
        "                extended_anchor_loss += weight * torch.mean(\n",
        "                    anchor_strength * (outputs[:, i] - current_val) ** 2\n",
        "                )\n",
        "            extended_anchor_loss = extended_anchor_loss / outputs.shape[1]\n",
        "            \n",
        "            # Combine losses: main loss + early rise detection + anchor + continuity + persistence\n",
        "            # ðŸ†• IMPROVED: Adjusted weights to fix mean-reversion bias\n",
        "            # - Moderate early_rise (0.3) to catch peaks\n",
        "            # - Extended anchor (0.1) applied across all horizons\n",
        "            # - Strong persistence (0.5) to prevent false upticks\n",
        "            # - Moderate continuity (1.0) for smooth join at t0\n",
        "            loss = (weighted_loss + \n",
        "                    0.3 * early_rise_loss + \n",
        "                    0.1 * extended_anchor_loss + \n",
        "                    0.5 * persistence_loss +\n",
        "                    1.0 * continuity_loss)\n",
        "            \n",
        "            # Check for NaN\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"\\nWarning: NaN loss detected at epoch {epoch+1}\")\n",
        "                raise ValueError(\"NaN loss encountered\")\n",
        "            \n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            train_losses.append(loss.item())\n",
        "            train_maes.append(torch.mean(torch.abs(outputs - batch_y)).item())\n",
        "            \n",
        "            # Update progress bar\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'mae': f'{torch.mean(torch.abs(outputs - batch_y)).item():.4f}'})\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val_tensor)\n",
        "            val_loss = criterion(val_outputs, y_val_tensor)\n",
        "            val_mae = torch.mean(torch.abs(val_outputs - y_val_tensor))\n",
        "        \n",
        "        # Record history\n",
        "        avg_train_loss = np.mean(train_losses)\n",
        "        avg_train_mae = np.mean(train_maes)\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['val_loss'].append(val_loss.item())\n",
        "        history['train_mae'].append(avg_train_mae)\n",
        "        history['val_mae'].append(val_mae.item())\n",
        "        \n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_loss)\n",
        "        \n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        \n",
        "        # Print progress\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}] ({epoch_time:.1f}s) - \"\n",
        "                  f\"Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss.item():.6f}, \"\n",
        "                  f\"Train MAE: {avg_train_mae:.6f}, Val MAE: {val_mae.item():.6f}\")\n",
        "        \n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "    \n",
        "    # Restore best model\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "        print(\"\\nâœ“ Restored best model weights\")\n",
        "    \n",
        "    print(\"âœ“ Model training complete!\")\n",
        "    \n",
        "    return model, scaler, history, sequence_length, horizons\n",
        "\n",
        "print(\"âœ“ Training function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Prepare Training Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing ISIS training data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/699919074.py:39: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  df_hourly = df.resample('1H').agg(aggregation_rules)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/699919074.py:39: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  df_hourly = df.resample('1H').agg(aggregation_rules)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning rainfall data for 13 stations...\n",
            "  Swindon: Capping 1 values >50mm/hour\n",
            "âœ“ Rainfall cleaning complete\n",
            "  Removing 45 spike values in differential\n",
            "Cleaning rainfall data for 13 stations...\n",
            "  Swindon: Capping 1 values >50mm/hour\n",
            "âœ“ Rainfall cleaning complete\n",
            "  Removing 45 spike values in differential\n",
            "\n",
            "âœ“ ISIS data prepared: (67315, 14)\n",
            "Date range: 2017-02-04 12:00:00+00:00 to 2026-01-20 10:00:00+00:00\n",
            "\n",
            "Creating features with future rainfall...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/3319942410.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['differential_range_72h'] = (\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/3319942410.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['differential_rolling_mean_168h'] = df[differential_column].rolling(window=168).mean()\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/3319942410.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['differential_rolling_max_168h'] = df[differential_column].rolling(window=168).max()\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/3319942410.py:193: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['differential_rolling_min_168h'] = df[differential_column].rolling(window=168).min()\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/3319942410.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['differential_range_168h'] = (\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/3319942410.py:199: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['antecedent_wetness'] = (\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/3319942410.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'catchment_rainfall_exp_{name}'] = (\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/3319942410.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'catchment_rainfall_exp_{name}'] = (\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/3319942410.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'catchment_rainfall_exp_{name}'] = (\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/3319942410.py:214: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['day_of_year'] = df.index.dayofyear\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/3319942410.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['day_of_year_sin'] = np.sin(2 * np.pi * df['day_of_year']/365.25)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/3319942410.py:216: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['day_of_year_cos'] = np.cos(2 * np.pi * df['day_of_year']/365.25)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n",
            "/var/folders/2_/s_gv2ntx7d1_wr0vktzyjyqw0000gp/T/ipykernel_72436/1921340240.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_with_features[target_col] = df_with_features[differential_column].shift(-horizon)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating targets for 24 horizons...\n",
            "\n",
            "======================================================================\n",
            "Multi-Horizon Model Setup:\n",
            "======================================================================\n",
            "Number of features: 111\n",
            "Number of horizons: 24\n",
            "Horizons: [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 30, 36, 42, 48, 72, 96, 120, 144, 168, 192, 216, 240]\n",
            "Training samples: 67075\n",
            "Date range: 2017-02-04 12:00:00+00:00 to 2026-01-10 10:00:00+00:00\n",
            "======================================================================\n",
            "\n",
            "Checking for data issues...\n",
            "X_isis contains NaN: True\n",
            "X_isis contains Inf: False\n",
            "y_isis_multi contains NaN: False\n",
            "Cleaning X_isis...\n",
            "âœ“ Cleaned\n",
            "\n",
            "âœ“ Data ready for training!\n"
          ]
        }
      ],
      "source": [
        "## 10. Prepare Training Data\n",
        "stretch = \"Isis\"\n",
        "# Merge and clean ISIS data\n",
        "\n",
        "\n",
        "print(\"Preparing ISIS training data...\")\n",
        "isis_df_featureless = merge_and_clean_data(\n",
        "    isis_hist_diff_df, \n",
        "    mega_rainfall_hist_df, \n",
        "    isis_api_diff_df, \n",
        "    mega_rainfall_api_df, \n",
        "    'differential'\n",
        ")\n",
        "\n",
        "isis_df_featureless = merge_and_clean_data(\n",
        "    isis_hist_diff_df, \n",
        "    mega_rainfall_hist_df, \n",
        "    isis_api_diff_df, \n",
        "    mega_rainfall_api_df, \n",
        "    'differential'\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ ISIS data prepared: {isis_df_featureless.shape}\")\n",
        "print(f\"Date range: {isis_df_featureless.index.min()} to {isis_df_featureless.index.max()}\")\n",
        "\n",
        "# Define prediction horizons:\n",
        "# - Every 2 hours for next 24 hours\n",
        "# - Every 6 hours for 24-48 hours  \n",
        "# - Every 24 hours (daily) for 48-240 hours\n",
        "horizons = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24,  # 0-24h: every 2h\n",
        "            30, 36, 42, 48,                               # 24-48h: every 6h\n",
        "            72, 96, 120, 144, 168, 192, 216, 240]         # 48-240h: every 24h (daily)\n",
        "\n",
        "# For TRAINING: Use the historical rainfall data itself as \"future\" rainfall\n",
        "# Extract just the rainfall columns from the historical data\n",
        "rainfall_cols = [col for col in isis_df_featureless.columns if col != 'differential']\n",
        "historical_rainfall_for_training = isis_df_featureless[rainfall_cols].copy()\n",
        "\n",
        "# Create multi-horizon targets and features\n",
        "X_isis, y_isis_multi, mask_isis, horizons = create_target_and_X_y_multihorizon(\n",
        "    isis_df_featureless,\n",
        "    historical_rainfall_for_training,  # Use historical data, not forecast!\n",
        "    differential_column='differential',\n",
        "    horizons=horizons\n",
        ")\n",
        "\n",
        "# Check for data issues\n",
        "print(\"\\nChecking for data issues...\")\n",
        "print(f\"X_isis contains NaN: {X_isis.isna().any().any()}\")\n",
        "print(f\"X_isis contains Inf: {np.isinf(X_isis.values).any()}\")\n",
        "print(f\"y_isis_multi contains NaN: {y_isis_multi.isna().any().any()}\")\n",
        "\n",
        "# Clean if needed\n",
        "if X_isis.isna().any().any() or np.isinf(X_isis.values).any():\n",
        "    print(\"Cleaning X_isis...\")\n",
        "    X_isis = X_isis.replace([np.inf, -np.inf], np.nan)\n",
        "    X_isis = X_isis.ffill().bfill().fillna(0)\n",
        "    print(\"âœ“ Cleaned\")\n",
        "\n",
        "if y_isis_multi.isna().any().any():\n",
        "    print(\"Cleaning y_isis_multi...\")\n",
        "    y_isis_multi = y_isis_multi.ffill().bfill().fillna(0)\n",
        "    print(\"âœ“ Cleaned\")\n",
        "\n",
        "print(\"\\nâœ“ Data ready for training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Bicester</th>\n",
              "      <th>Bourton</th>\n",
              "      <th>Byfield</th>\n",
              "      <th>Chipping</th>\n",
              "      <th>Eynsham</th>\n",
              "      <th>Grimsbury</th>\n",
              "      <th>Osney</th>\n",
              "      <th>Rapsgate</th>\n",
              "      <th>Shorncote</th>\n",
              "      <th>St</th>\n",
              "      <th>...</th>\n",
              "      <th>differential_rolling_max_168h</th>\n",
              "      <th>differential_rolling_min_168h</th>\n",
              "      <th>differential_range_168h</th>\n",
              "      <th>antecedent_wetness</th>\n",
              "      <th>catchment_rainfall_exp_fast</th>\n",
              "      <th>catchment_rainfall_exp_medium</th>\n",
              "      <th>catchment_rainfall_exp_slow</th>\n",
              "      <th>day_of_year</th>\n",
              "      <th>day_of_year_sin</th>\n",
              "      <th>day_of_year_cos</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-02-04 12:00:00+00:00</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.603580</td>\n",
              "      <td>0.156300</td>\n",
              "      <td>0.447280</td>\n",
              "      <td>7.755</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>35</td>\n",
              "      <td>0.566362</td>\n",
              "      <td>0.824157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-04 13:00:00+00:00</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.603580</td>\n",
              "      <td>0.156300</td>\n",
              "      <td>0.447280</td>\n",
              "      <td>7.755</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>35</td>\n",
              "      <td>0.566362</td>\n",
              "      <td>0.824157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-04 14:00:00+00:00</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.603580</td>\n",
              "      <td>0.156300</td>\n",
              "      <td>0.447280</td>\n",
              "      <td>7.755</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>35</td>\n",
              "      <td>0.566362</td>\n",
              "      <td>0.824157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-04 15:00:00+00:00</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.603580</td>\n",
              "      <td>0.156300</td>\n",
              "      <td>0.447280</td>\n",
              "      <td>7.755</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>35</td>\n",
              "      <td>0.566362</td>\n",
              "      <td>0.824157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-04 16:00:00+00:00</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.603580</td>\n",
              "      <td>0.156300</td>\n",
              "      <td>0.447280</td>\n",
              "      <td>7.755</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>35</td>\n",
              "      <td>0.566362</td>\n",
              "      <td>0.824157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2026-01-10 06:00:00+00:00</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.426772</td>\n",
              "      <td>0.142698</td>\n",
              "      <td>0.284075</td>\n",
              "      <td>149.650</td>\n",
              "      <td>0.007688</td>\n",
              "      <td>1.419657</td>\n",
              "      <td>2.956617</td>\n",
              "      <td>10</td>\n",
              "      <td>0.171177</td>\n",
              "      <td>0.985240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2026-01-10 07:00:00+00:00</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.426772</td>\n",
              "      <td>0.142698</td>\n",
              "      <td>0.284075</td>\n",
              "      <td>144.776</td>\n",
              "      <td>0.005491</td>\n",
              "      <td>1.306084</td>\n",
              "      <td>2.875614</td>\n",
              "      <td>10</td>\n",
              "      <td>0.171177</td>\n",
              "      <td>0.985240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2026-01-10 08:00:00+00:00</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.426772</td>\n",
              "      <td>0.142698</td>\n",
              "      <td>0.284075</td>\n",
              "      <td>142.266</td>\n",
              "      <td>0.003922</td>\n",
              "      <td>1.201597</td>\n",
              "      <td>2.796830</td>\n",
              "      <td>10</td>\n",
              "      <td>0.171177</td>\n",
              "      <td>0.985240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2026-01-10 09:00:00+00:00</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.426772</td>\n",
              "      <td>0.142698</td>\n",
              "      <td>0.284075</td>\n",
              "      <td>141.001</td>\n",
              "      <td>0.002802</td>\n",
              "      <td>1.105470</td>\n",
              "      <td>2.720204</td>\n",
              "      <td>10</td>\n",
              "      <td>0.171177</td>\n",
              "      <td>0.985240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2026-01-10 10:00:00+00:00</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.427355</td>\n",
              "      <td>0.142698</td>\n",
              "      <td>0.284658</td>\n",
              "      <td>140.978</td>\n",
              "      <td>0.022001</td>\n",
              "      <td>1.022632</td>\n",
              "      <td>2.647596</td>\n",
              "      <td>10</td>\n",
              "      <td>0.171177</td>\n",
              "      <td>0.985240</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>67075 rows Ã— 111 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                           Bicester  Bourton  Byfield  Chipping  Eynsham  \\\n",
              "timestamp                                                                  \n",
              "2017-02-04 12:00:00+00:00       0.0      0.0      0.0       0.0      0.0   \n",
              "2017-02-04 13:00:00+00:00       0.0      0.0      0.0       0.0      0.0   \n",
              "2017-02-04 14:00:00+00:00       0.0      0.0      0.0       0.0      0.0   \n",
              "2017-02-04 15:00:00+00:00       0.0      0.0      0.0       0.0      0.0   \n",
              "2017-02-04 16:00:00+00:00       0.0      0.0      0.0       0.0      0.0   \n",
              "...                             ...      ...      ...       ...      ...   \n",
              "2026-01-10 06:00:00+00:00       0.0      0.0      0.0       0.0      0.0   \n",
              "2026-01-10 07:00:00+00:00       0.0      0.0      0.0       0.0      0.0   \n",
              "2026-01-10 08:00:00+00:00       0.0      0.0      0.0       0.0      0.0   \n",
              "2026-01-10 09:00:00+00:00       0.0      0.0      0.0       0.0      0.0   \n",
              "2026-01-10 10:00:00+00:00       0.0      0.0      0.0       0.0      0.0   \n",
              "\n",
              "                           Grimsbury  Osney  Rapsgate  Shorncote   St  ...  \\\n",
              "timestamp                                                              ...   \n",
              "2017-02-04 12:00:00+00:00        0.0    0.0       0.0        0.0  0.0  ...   \n",
              "2017-02-04 13:00:00+00:00        0.0    0.0       0.0        0.0  0.0  ...   \n",
              "2017-02-04 14:00:00+00:00        0.0    0.0       0.0        0.0  0.0  ...   \n",
              "2017-02-04 15:00:00+00:00        0.0    0.0       0.0        0.0  0.0  ...   \n",
              "2017-02-04 16:00:00+00:00        0.0    0.0       0.0        0.0  0.0  ...   \n",
              "...                              ...    ...       ...        ...  ...  ...   \n",
              "2026-01-10 06:00:00+00:00        0.0    0.0       0.0        0.0  0.0  ...   \n",
              "2026-01-10 07:00:00+00:00        0.0    0.0       0.0        0.0  0.0  ...   \n",
              "2026-01-10 08:00:00+00:00        0.0    0.0       0.0        0.0  0.0  ...   \n",
              "2026-01-10 09:00:00+00:00        0.0    0.0       0.0        0.0  0.0  ...   \n",
              "2026-01-10 10:00:00+00:00        0.0    0.0       0.0        0.0  0.0  ...   \n",
              "\n",
              "                           differential_rolling_max_168h  \\\n",
              "timestamp                                                  \n",
              "2017-02-04 12:00:00+00:00                       0.603580   \n",
              "2017-02-04 13:00:00+00:00                       0.603580   \n",
              "2017-02-04 14:00:00+00:00                       0.603580   \n",
              "2017-02-04 15:00:00+00:00                       0.603580   \n",
              "2017-02-04 16:00:00+00:00                       0.603580   \n",
              "...                                                  ...   \n",
              "2026-01-10 06:00:00+00:00                       0.426772   \n",
              "2026-01-10 07:00:00+00:00                       0.426772   \n",
              "2026-01-10 08:00:00+00:00                       0.426772   \n",
              "2026-01-10 09:00:00+00:00                       0.426772   \n",
              "2026-01-10 10:00:00+00:00                       0.427355   \n",
              "\n",
              "                           differential_rolling_min_168h  \\\n",
              "timestamp                                                  \n",
              "2017-02-04 12:00:00+00:00                       0.156300   \n",
              "2017-02-04 13:00:00+00:00                       0.156300   \n",
              "2017-02-04 14:00:00+00:00                       0.156300   \n",
              "2017-02-04 15:00:00+00:00                       0.156300   \n",
              "2017-02-04 16:00:00+00:00                       0.156300   \n",
              "...                                                  ...   \n",
              "2026-01-10 06:00:00+00:00                       0.142698   \n",
              "2026-01-10 07:00:00+00:00                       0.142698   \n",
              "2026-01-10 08:00:00+00:00                       0.142698   \n",
              "2026-01-10 09:00:00+00:00                       0.142698   \n",
              "2026-01-10 10:00:00+00:00                       0.142698   \n",
              "\n",
              "                           differential_range_168h  antecedent_wetness  \\\n",
              "timestamp                                                                \n",
              "2017-02-04 12:00:00+00:00                 0.447280               7.755   \n",
              "2017-02-04 13:00:00+00:00                 0.447280               7.755   \n",
              "2017-02-04 14:00:00+00:00                 0.447280               7.755   \n",
              "2017-02-04 15:00:00+00:00                 0.447280               7.755   \n",
              "2017-02-04 16:00:00+00:00                 0.447280               7.755   \n",
              "...                                            ...                 ...   \n",
              "2026-01-10 06:00:00+00:00                 0.284075             149.650   \n",
              "2026-01-10 07:00:00+00:00                 0.284075             144.776   \n",
              "2026-01-10 08:00:00+00:00                 0.284075             142.266   \n",
              "2026-01-10 09:00:00+00:00                 0.284075             141.001   \n",
              "2026-01-10 10:00:00+00:00                 0.284658             140.978   \n",
              "\n",
              "                           catchment_rainfall_exp_fast  \\\n",
              "timestamp                                                \n",
              "2017-02-04 12:00:00+00:00                     0.000000   \n",
              "2017-02-04 13:00:00+00:00                     0.000000   \n",
              "2017-02-04 14:00:00+00:00                     0.000000   \n",
              "2017-02-04 15:00:00+00:00                     0.000000   \n",
              "2017-02-04 16:00:00+00:00                     0.000000   \n",
              "...                                                ...   \n",
              "2026-01-10 06:00:00+00:00                     0.007688   \n",
              "2026-01-10 07:00:00+00:00                     0.005491   \n",
              "2026-01-10 08:00:00+00:00                     0.003922   \n",
              "2026-01-10 09:00:00+00:00                     0.002802   \n",
              "2026-01-10 10:00:00+00:00                     0.022001   \n",
              "\n",
              "                           catchment_rainfall_exp_medium  \\\n",
              "timestamp                                                  \n",
              "2017-02-04 12:00:00+00:00                       0.000000   \n",
              "2017-02-04 13:00:00+00:00                       0.000000   \n",
              "2017-02-04 14:00:00+00:00                       0.000000   \n",
              "2017-02-04 15:00:00+00:00                       0.000000   \n",
              "2017-02-04 16:00:00+00:00                       0.000000   \n",
              "...                                                  ...   \n",
              "2026-01-10 06:00:00+00:00                       1.419657   \n",
              "2026-01-10 07:00:00+00:00                       1.306084   \n",
              "2026-01-10 08:00:00+00:00                       1.201597   \n",
              "2026-01-10 09:00:00+00:00                       1.105470   \n",
              "2026-01-10 10:00:00+00:00                       1.022632   \n",
              "\n",
              "                           catchment_rainfall_exp_slow  day_of_year  \\\n",
              "timestamp                                                             \n",
              "2017-02-04 12:00:00+00:00                     0.000000           35   \n",
              "2017-02-04 13:00:00+00:00                     0.000000           35   \n",
              "2017-02-04 14:00:00+00:00                     0.000000           35   \n",
              "2017-02-04 15:00:00+00:00                     0.000000           35   \n",
              "2017-02-04 16:00:00+00:00                     0.000000           35   \n",
              "...                                                ...          ...   \n",
              "2026-01-10 06:00:00+00:00                     2.956617           10   \n",
              "2026-01-10 07:00:00+00:00                     2.875614           10   \n",
              "2026-01-10 08:00:00+00:00                     2.796830           10   \n",
              "2026-01-10 09:00:00+00:00                     2.720204           10   \n",
              "2026-01-10 10:00:00+00:00                     2.647596           10   \n",
              "\n",
              "                           day_of_year_sin  day_of_year_cos  \n",
              "timestamp                                                    \n",
              "2017-02-04 12:00:00+00:00         0.566362         0.824157  \n",
              "2017-02-04 13:00:00+00:00         0.566362         0.824157  \n",
              "2017-02-04 14:00:00+00:00         0.566362         0.824157  \n",
              "2017-02-04 15:00:00+00:00         0.566362         0.824157  \n",
              "2017-02-04 16:00:00+00:00         0.566362         0.824157  \n",
              "...                                    ...              ...  \n",
              "2026-01-10 06:00:00+00:00         0.171177         0.985240  \n",
              "2026-01-10 07:00:00+00:00         0.171177         0.985240  \n",
              "2026-01-10 08:00:00+00:00         0.171177         0.985240  \n",
              "2026-01-10 09:00:00+00:00         0.171177         0.985240  \n",
              "2026-01-10 10:00:00+00:00         0.171177         0.985240  \n",
              "\n",
              "[67075 rows x 111 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_isis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Train Multi-Horizon Model\n",
        "\n",
        "This will take a few minutes depending on your hardware (faster on GPU/MPS).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸš€ READY TO RETRAIN WITH IMPROVEMENTS!\n",
        "\n",
        "The model has been enhanced with:\n",
        "\n",
        "### ðŸ†• FIX: Low-Flow Stability Features (prevents false upticks)\n",
        "- **hours_since_rain**: Time since last significant rainfall\n",
        "- **is_dry_spell**: Binary flag for 48+ hours without rain\n",
        "- **stable_low_regime**: Combined low-flow + dry-spell indicator\n",
        "- **future_is_dry_XXh**: Whether future rainfall is minimal\n",
        "- **expect_stable_low**: Strong signal that river should stay low\n",
        "\n",
        "### ðŸ†• FIX: Improved Loss Function\n",
        "1. **Persistence Loss** (NEW): Penalizes predicting upticks when actual stays low\n",
        "2. **Extended Anchor Loss** (IMPROVED): Applied to ALL horizons, not just first 6\n",
        "3. **Asymmetric Weighting** (NEW): Doesn't over-penalize staying near current low value\n",
        "4. **Weighted MSE**: High-flow (3x), Rising events (1.5x)\n",
        "5. **Early Rise Detection**: Catches flood peaks earlier\n",
        "\n",
        "### Loss Function Formula:\n",
        "```\n",
        "loss = weighted_mse + 0.3*early_rise + 0.1*extended_anchor + 0.5*persistence + 1.0*continuity\n",
        "```\n",
        "\n",
        "**Next steps:**\n",
        "- Run Cell 20 to prepare training data with new features\n",
        "- Run Cell 23 to train the improved model\n",
        "- Re-run the what-if forecasts to verify the fix!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sequence shape: (66955, 120, 111)\n",
            "Target shape: (66955, 24)\n",
            "Horizon importance weights: [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.5, 1.5, 1.5, 1.5, 1.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "\n",
            "Model Architecture:\n",
            "MultiHorizonLSTMModel(\n",
            "  (lstm_layers): ModuleList(\n",
            "    (0): LSTM(111, 192, batch_first=True)\n",
            "    (1): LSTM(192, 128, batch_first=True)\n",
            "    (2): LSTM(128, 64, batch_first=True)\n",
            "  )\n",
            "  (dropout_layers): ModuleList(\n",
            "    (0-2): 3 x Dropout(p=0.3, inplace=False)\n",
            "  )\n",
            "  (fc_layers): ModuleList(\n",
            "    (0-23): 24 x Linear(in_features=64, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "======================================================================\n",
            "Training Configuration:\n",
            "======================================================================\n",
            "Device: mps\n",
            "Training samples: 53564\n",
            "Validation samples: 13391\n",
            "Batch size: 64\n",
            "Epochs: 30\n",
            "======================================================================\n",
            "\n",
            "[DEBUG] Creating loss criterion...\n",
            "[DEBUG] âœ“ Loss criterion created\n",
            "[DEBUG] Creating optimizer...\n",
            "[DEBUG] âœ“ Optimizer created\n",
            "[DEBUG] Creating learning rate scheduler...\n",
            "[DEBUG] âœ“ Scheduler created\n",
            "[DEBUG] Initializing training variables...\n",
            "[DEBUG] âœ“ Variables initialized\n",
            "\n",
            "======================================================================\n",
            "STARTING TRAINING\n",
            "======================================================================\n",
            "\n",
            "[DEBUG] Getting first batch from DataLoader...\n",
            "[DEBUG] âœ“ Got first batch\n",
            "[DEBUG] Moving batch to device (mps)...\n",
            "[DEBUG] âœ“ Batch moved to mps\n",
            "[DEBUG] Setting model to training mode...\n",
            "[DEBUG] âœ“ Model in training mode\n",
            "[DEBUG] Running forward pass...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/robertds413/Documents/Flag_Predictor/.venv_new/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] âœ“ Forward pass complete, output shape: torch.Size([64, 24])\n",
            "[DEBUG] Computing loss...\n",
            "[DEBUG] âœ“ Loss computed: 0.125022\n",
            "\n",
            "======================================================================\n",
            "âœ“ FIRST BATCH TEST PASSED! Loss: 0.125022\n",
            "======================================================================\n",
            "\n",
            "[DEBUG] Starting epoch loop...\n",
            "\n",
            "======================================================================\n",
            "EPOCH 1/30\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/30:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 297/837 [00:33<01:01,  8.79it/s, loss=0.0895, mae=0.0877]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 102\u001b[39m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ“ Model ready for predictions!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     88\u001b[39m     \u001b[38;5;66;03m# Train the improved multi-horizon model with enhanced features and weighted loss\u001b[39;00m\n\u001b[32m     89\u001b[39m     \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     99\u001b[39m     \u001b[38;5;66;03m# - Added anchor loss to reduce overdispersion\u001b[39;00m\n\u001b[32m    100\u001b[39m     \u001b[38;5;66;03m# - Added continuity loss to ensure smooth transition from observed to predicted\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     model_tuple = \u001b[43mtrain_multihorizon_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_isis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_isis_multi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhorizons\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m120\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# ðŸ†• 5 days of history (was 72) for better pattern recognition\u001b[39;49;00m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# Increased from 10 for better convergence with new loss\u001b[39;49;00m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# Larger batch for speed (was 32)\u001b[39;49;00m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Keep conservative\u001b[39;49;00m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Early stopping if no improvement\u001b[39;49;00m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ðŸ†• Deeper 3-layer LSTM with more capacity\u001b[39;49;00m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# ðŸ†• Slightly stronger dropout\u001b[39;49;00m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     best_model, scaler, history, sequence_length, horizons = model_tuple\n\u001b[32m    117\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâœ“ Training complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 179\u001b[39m, in \u001b[36mtrain_multihorizon_model\u001b[39m\u001b[34m(X, y_multi, horizons, sequence_length, epochs, batch_size, validation_split, learning_rate, patience, max_grad_norm, hidden_sizes, dropout_rate)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# Progress bar\u001b[39;00m\n\u001b[32m    177\u001b[39m pbar = tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m, leave=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Move batch to device (MPS/CUDA/CPU)\u001b[39;49;00m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Flag_Predictor/.venv_new/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Flag_Predictor/.venv_new/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    635\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Flag_Predictor/.venv_new/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    674\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    676\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    677\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Flag_Predictor/.venv_new/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Flag_Predictor/.venv_new/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:277\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    217\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    218\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    219\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Flag_Predictor/.venv_new/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:144\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    141\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Flag_Predictor/.venv_new/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:121\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m    124\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Flag_Predictor/.venv_new/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:174\u001b[39m, in \u001b[36mcollate_tensor_fn\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    172\u001b[39m     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n\u001b[32m    173\u001b[39m     out = elem.new(storage).resize_(\u001b[38;5;28mlen\u001b[39m(batch), *\u001b[38;5;28mlist\u001b[39m(elem.size()))\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# SKIP TRAINING FLAG: Set to True to load the latest saved model instead\n",
        "# ============================================================================\n",
        "# ðŸ†• FIX APPLIED: Set to False to retrain with improvements that fix the\n",
        "# mean-reversion bias (false upticks at end of forecast when values are low)\n",
        "#\n",
        "# KEY IMPROVEMENTS:\n",
        "# 1. LOW-FLOW REGIME FEATURES: hours_since_rain, is_dry_spell, stable_low_regime\n",
        "# 2. FUTURE DRY FEATURES: future_is_dry_XXh, expect_stable_low\n",
        "# 3. PERSISTENCE LOSS: Penalizes predicting rises when actual stays low\n",
        "# 4. EXTENDED ANCHOR LOSS: Applies to ALL horizons (not just first 6)\n",
        "# 5. ASYMMETRIC WEIGHTING: Doesn't over-penalize staying near current low value\n",
        "# ============================================================================\n",
        "SKIP_TRAINING = False  # Change to True after retraining to skip in future\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import glob\n",
        "\n",
        "if SKIP_TRAINING:\n",
        "    print(\"=\"*70)\n",
        "    print(\"SKIPPING TRAINING - Loading latest saved model\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Check if models directory exists\n",
        "    models_dir = '../models'\n",
        "    latest_model_path = os.path.join(models_dir, 'multihorizon_model_latest.pth')\n",
        "    latest_scaler_path = os.path.join(models_dir, 'scaler_latest.pkl')\n",
        "    latest_config_path = os.path.join(models_dir, 'config_latest.pkl')\n",
        "    \n",
        "    # Try to find the latest model (either latest_* or most recent timestamped)\n",
        "    if not os.path.exists(latest_model_path):\n",
        "        # Look for most recent timestamped model\n",
        "        model_files = glob.glob(os.path.join(models_dir, 'multihorizon_model_*.pth'))\n",
        "        if model_files:\n",
        "            # Sort by modification time, get most recent\n",
        "            latest_model_path = max(model_files, key=os.path.getmtime)\n",
        "            # Find corresponding scaler and config\n",
        "            base_name = os.path.basename(latest_model_path).replace('multihorizon_model_', '').replace('.pth', '')\n",
        "            latest_scaler_path = os.path.join(models_dir, f'scaler_{base_name}.pkl')\n",
        "            latest_config_path = os.path.join(models_dir, f'config_{base_name}.pkl')\n",
        "            print(f\"  Found timestamped model: {os.path.basename(latest_model_path)}\")\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"No saved model found in {models_dir}. Please train a model first or set SKIP_TRAINING=False\")\n",
        "    \n",
        "    # Load configuration\n",
        "    if not os.path.exists(latest_config_path):\n",
        "        raise FileNotFoundError(f\"Config file not found: {latest_config_path}\")\n",
        "    \n",
        "    with open(latest_config_path, 'rb') as f:\n",
        "        config = pickle.load(f)\n",
        "    \n",
        "    # Extract parameters from config\n",
        "    sequence_length = config.get('sequence_length', 120)\n",
        "    horizons = config.get('horizons', [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24,\n",
        "                                        30, 36, 42, 48, 72, 96, 120, 144, 168, 192, 216, 240])\n",
        "    \n",
        "    # Recreate model architecture\n",
        "    n_features = config['input_size']\n",
        "    n_horizons = len(horizons)\n",
        "    best_model = MultiHorizonLSTMModel(\n",
        "        input_size=n_features,\n",
        "        hidden_sizes=config['hidden_sizes'],\n",
        "        n_horizons=n_horizons,\n",
        "        dropout_rate=config['dropout_rate']\n",
        "    )\n",
        "    best_model = best_model.to(device)\n",
        "    \n",
        "    # Load weights\n",
        "    best_model.load_state_dict(torch.load(latest_model_path, map_location=device))\n",
        "    best_model.eval()  # Set to evaluation mode\n",
        "    \n",
        "    # Load scaler\n",
        "    if not os.path.exists(latest_scaler_path):\n",
        "        raise FileNotFoundError(f\"Scaler file not found: {latest_scaler_path}\")\n",
        "    \n",
        "    with open(latest_scaler_path, 'rb') as f:\n",
        "        scaler = pickle.load(f)\n",
        "    \n",
        "    print(f\"\\nâœ“ Model loaded from: {latest_model_path}\")\n",
        "    print(f\"âœ“ Scaler loaded from: {latest_scaler_path}\")\n",
        "    print(f\"âœ“ Config loaded from: {latest_config_path}\")\n",
        "    print(f\"âœ“ Sequence length: {sequence_length}\")\n",
        "    print(f\"âœ“ Horizons: {horizons}\")\n",
        "    print(f\"âœ“ Model ready for predictions!\")\n",
        "    \n",
        "else:\n",
        "    # Train the improved multi-horizon model with enhanced features and weighted loss\n",
        "    # \n",
        "    # HYPERPARAMETER GUIDE (for hourly data with 24 horizons):\n",
        "    # - sequence_length: 72h (3 days) - captures recent weather patterns & river response\n",
        "    # - epochs: 30 (with early stopping, anchor loss helps faster convergence)\n",
        "    # - batch_size: 64 (larger batch = faster training + more stable gradients)\n",
        "    # - learning_rate: 0.0001 (conservative, won't overshoot with weighted loss)\n",
        "    # - patience: 7 (early stopping for efficient training)\n",
        "    # \n",
        "    # LOSS FUNCTION: weighted_loss + 0.3*early_rise + 0.1*anchor + 2.0*continuity\n",
        "    # - Removed smoothness regularization to allow sharper predictions\n",
        "    # - Added anchor loss to reduce overdispersion\n",
        "    # - Added continuity loss to ensure smooth transition from observed to predicted\n",
        "\n",
        "    model_tuple = train_multihorizon_model(\n",
        "        X_isis, \n",
        "        y_isis_multi, \n",
        "        horizons,\n",
        "        sequence_length=120,       # ðŸ†• 5 days of history (was 72) for better pattern recognition\n",
        "        epochs=30,                 # Increased from 10 for better convergence with new loss\n",
        "        batch_size=64,             # Larger batch for speed (was 32)\n",
        "        learning_rate=0.0001,      # Keep conservative\n",
        "        patience=7,                # Early stopping if no improvement\n",
        "        hidden_sizes=[192, 128, 64],  # ðŸ†• Deeper 3-layer LSTM with more capacity\n",
        "        dropout_rate=0.3           # ðŸ†• Slightly stronger dropout\n",
        "    )\n",
        "\n",
        "    best_model, scaler, history, sequence_length, horizons = model_tuple\n",
        "\n",
        "    print(\"\\nâœ“ Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Save the trained model weights and training artifacts\n",
        "# import os\n",
        "# from datetime import datetime\n",
        "\n",
        "# # Create models directory if it doesn't exist\n",
        "# models_dir = '../models'\n",
        "# os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "# # Generate timestamp for unique filename\n",
        "# timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "# model_path = os.path.join(models_dir, f'multihorizon_model_{timestamp}.pth')\n",
        "# scaler_path = os.path.join(models_dir, f'scaler_{timestamp}.pkl')\n",
        "# config_path = os.path.join(models_dir, f'config_{timestamp}.pkl')\n",
        "\n",
        "# # Save model weights\n",
        "# torch.save(best_model.state_dict(), model_path)\n",
        "# print(f\"âœ“ Model weights saved to: {model_path}\")\n",
        "\n",
        "# # Save scaler\n",
        "# import pickle\n",
        "# with open(scaler_path, 'wb') as f:\n",
        "#     pickle.dump(scaler, f)\n",
        "# print(f\"âœ“ Scaler saved to: {scaler_path}\")\n",
        "\n",
        "# # Save configuration (for reproducibility)\n",
        "# config = {\n",
        "#     'sequence_length': sequence_length,\n",
        "#     'horizons': horizons,\n",
        "#     'hidden_sizes': [192, 128, 64],\n",
        "#     'dropout_rate': 0.3,\n",
        "#     'input_size': len(X_isis.columns),\n",
        "#     'feature_columns': list(X_isis.columns),\n",
        "#     'training_history': history\n",
        "# }\n",
        "# with open(config_path, 'wb') as f:\n",
        "#     pickle.dump(config, f)\n",
        "# print(f\"âœ“ Configuration saved to: {config_path}\")\n",
        "\n",
        "# # Also save a \"latest\" version for easy loading\n",
        "# latest_model_path = os.path.join(models_dir, 'multihorizon_model_latest.pth')\n",
        "# latest_scaler_path = os.path.join(models_dir, 'scaler_latest.pkl')\n",
        "# latest_config_path = os.path.join(models_dir, 'config_latest.pkl')\n",
        "\n",
        "# torch.save(best_model.state_dict(), latest_model_path)\n",
        "# with open(latest_scaler_path, 'wb') as f:\n",
        "#     pickle.dump(scaler, f)\n",
        "# with open(latest_config_path, 'wb') as f:\n",
        "#     pickle.dump(config, f)\n",
        "# print(f\"\\nâœ“ Latest versions saved for easy loading\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to load a saved model\n",
        "def load_trained_model(model_path=None, scaler_path=None, config_path=None):\n",
        "    \"\"\"\n",
        "    Load a previously trained model, scaler, and configuration.\n",
        "    \n",
        "    Args:\n",
        "        model_path: Path to model weights (default: latest)\n",
        "        scaler_path: Path to scaler (default: latest)\n",
        "        config_path: Path to config (default: latest)\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (model, scaler, config)\n",
        "    \"\"\"\n",
        "    import pickle\n",
        "    from torch import nn\n",
        "    \n",
        "    # Use latest versions if paths not specified\n",
        "    if model_path is None:\n",
        "        model_path = os.path.join(models_dir, 'multihorizon_model_2025_08.pth')\n",
        "    if scaler_path is None:\n",
        "        scaler_path = os.path.join(models_dir, 'scaler_2025_08.pkl')\n",
        "    if config_path is None:\n",
        "        config_path = os.path.join(models_dir, 'config_2025_08.pkl')\n",
        "    \n",
        "    # Load configuration\n",
        "    with open(config_path, 'rb') as f:\n",
        "        config = pickle.load(f)\n",
        "    \n",
        "    # Recreate model architecture - MultiHorizonLSTMModel must be defined earlier in the notebook\n",
        "    model = MultiHorizonLSTMModel(\n",
        "        input_size=config['input_size'],\n",
        "        hidden_sizes=config['hidden_sizes'],\n",
        "        n_horizons=len(config['horizons']),\n",
        "        dropout_rate=config['dropout_rate']\n",
        "    )\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Load weights\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    \n",
        "    # Load scaler\n",
        "    with open(scaler_path, 'rb') as f:\n",
        "        scaler = pickle.load(f)\n",
        "    \n",
        "    print(f\"âœ“ Model loaded from: {model_path}\")\n",
        "    print(f\"âœ“ Scaler loaded from: {scaler_path}\")\n",
        "    print(f\"âœ“ Config loaded from: {config_path}\")\n",
        "    \n",
        "    return model, scaler, config\n",
        "\n",
        "print(\"\\nðŸ“¦ Use load_trained_model() to reload this model in future sessions\")\n",
        "\n",
        "# Don't call load_trained_model() here - it will fail if MultiHorizonLSTM isn't defined yet\n",
        "# Users should call it manually when needed: model, scaler, config = load_trained_model()\n",
        "load_trained_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Generate Future Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def predict_single_ensemble_member(model, scaler, historical_df, rainfall_forecast_df, \n",
        "                                    X_train_columns, sequence_length=72, \n",
        "                                    horizons=None, verbose=True):\n",
        "    \"\"\"\n",
        "    Generate a 240-hour river flow prediction for ONE ensemble rainfall member.\n",
        "    \n",
        "    This function:\n",
        "    1. Combines historical data with the FULL future rainfall forecast\n",
        "    2. Computes features ONCE with future rainfall visible\n",
        "    3. Makes ONE prediction from the current state\n",
        "    4. Interpolates the sparse horizon predictions into a continuous timeseries\n",
        "    \n",
        "    Args:\n",
        "        model: Trained PyTorch model\n",
        "        scaler: Fitted scaler\n",
        "        historical_df: Historical data with differential and rainfall\n",
        "        rainfall_forecast_df: 240-hour rainfall forecast for this ensemble member\n",
        "        X_train_columns: Column names used in training\n",
        "        sequence_length: LSTM sequence length (default 72 = 3 days)\n",
        "        horizons: List of prediction horizons [2, 4, 6, ..., 240]\n",
        "        verbose: Whether to print progress\n",
        "    \n",
        "    Returns:\n",
        "        pd.Series: 240-hour river differential prediction (hourly)\n",
        "    \"\"\"\n",
        "    if horizons is None:\n",
        "        horizons = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24,\n",
        "                    30, 36, 42, 48,\n",
        "                    72, 96, 120, 144, 168, 192, 216, 240]\n",
        "    \n",
        "    # Ensure timezone alignment\n",
        "    if rainfall_forecast_df.index.tz is None and historical_df.index.tz is not None:\n",
        "        rainfall_forecast_df = rainfall_forecast_df.tz_localize(historical_df.index.tz)\n",
        "    elif rainfall_forecast_df.index.tz is not None and historical_df.index.tz is None:\n",
        "        rainfall_forecast_df = rainfall_forecast_df.tz_convert('UTC').tz_localize(None)\n",
        "        historical_df = historical_df.copy()\n",
        "    \n",
        "    # Get enough historical data for rolling features (30 days = 720 hours)\n",
        "    lookback_hours = 720\n",
        "    recent_history = historical_df.iloc[-lookback_hours:].copy()\n",
        "    \n",
        "    # Store the forecast start time and current differential\n",
        "    forecast_start_time = recent_history.index[-1]\n",
        "    current_differential = recent_history['differential'].iloc[-1]\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"  Current time: {forecast_start_time}\")\n",
        "        print(f\"  Current differential: {current_differential:.3f}m\")\n",
        "    \n",
        "    # Create combined dataframe: historical + future rainfall\n",
        "    # This allows the feature engineering to \"see\" future rainfall\n",
        "    combined_df = recent_history.copy()\n",
        "    \n",
        "    # Add future rainfall data to the combined dataframe\n",
        "    for col in combined_df.columns:\n",
        "        if col in rainfall_forecast_df.columns:\n",
        "            # Append future rainfall\n",
        "            future_data = rainfall_forecast_df[[col]].copy()\n",
        "            # Only add future timestamps not already in combined_df\n",
        "            future_data = future_data[future_data.index > combined_df.index[-1]]\n",
        "            combined_df = pd.concat([combined_df, future_data])\n",
        "        elif col == 'differential':\n",
        "            # Forward fill differential for future timestamps\n",
        "            future_times = rainfall_forecast_df.index[rainfall_forecast_df.index > combined_df.index[-1]]\n",
        "            future_diff = pd.DataFrame(\n",
        "                {'differential': [current_differential] * len(future_times)},\n",
        "                index=future_times\n",
        "            )\n",
        "            combined_df = pd.concat([combined_df, future_diff])\n",
        "    \n",
        "    # Remove any duplicate indices\n",
        "    combined_df = combined_df[~combined_df.index.duplicated(keep='first')]\n",
        "    combined_df = combined_df.sort_index()\n",
        "    \n",
        "    # Now create features with the FULL future rainfall visible\n",
        "    # This is the key fix - features are computed with future data available\n",
        "    df_with_features = create_features_with_future_rainfall(\n",
        "        combined_df,\n",
        "        rainfall_forecast_df,\n",
        "        differential_column='differential'\n",
        "    )\n",
        "    \n",
        "    # Get features at the \"current\" time (end of historical data)\n",
        "    # This is where we make our prediction FROM\n",
        "    current_time_idx = df_with_features.index.get_loc(forecast_start_time)\n",
        "    \n",
        "    # Get only the features used during training (in same order)\n",
        "    X_forecast = df_with_features[X_train_columns].copy()\n",
        "    \n",
        "    # Fill any remaining NaN (should be minimal now)\n",
        "    X_forecast = X_forecast.ffill().bfill().fillna(0)\n",
        "    \n",
        "    # Get the input sequence (last `sequence_length` hours before current time)\n",
        "    sequence_start = current_time_idx - sequence_length + 1\n",
        "    sequence_end = current_time_idx + 1\n",
        "    sequence_data = X_forecast.iloc[sequence_start:sequence_end]\n",
        "    \n",
        "    if len(sequence_data) < sequence_length:\n",
        "        raise ValueError(f\"Not enough historical data. Need {sequence_length} hours, got {len(sequence_data)}\")\n",
        "    \n",
        "    # Scale and convert to tensor\n",
        "    sequence_scaled = scaler.transform(sequence_data)\n",
        "    sequence_tensor = torch.FloatTensor(sequence_scaled).unsqueeze(0).to(device)\n",
        "    \n",
        "    # Make prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(sequence_tensor).cpu().numpy()[0]  # Shape: (n_horizons,)\n",
        "    \n",
        "    # Create sparse prediction series (at model horizons)\n",
        "    horizon_times = [forecast_start_time + pd.Timedelta(hours=h) for h in horizons]\n",
        "    sparse_predictions = pd.Series(predictions, index=horizon_times)\n",
        "    \n",
        "    # Add current value at t=0 for interpolation\n",
        "    sparse_predictions[forecast_start_time] = current_differential\n",
        "    sparse_predictions = sparse_predictions.sort_index()\n",
        "    \n",
        "    # Interpolate to hourly resolution for full 240-hour forecast\n",
        "    full_timeline = pd.date_range(\n",
        "        start=forecast_start_time,\n",
        "        periods=241,  # 0 to 240 hours inclusive\n",
        "        freq='1h'\n",
        "    )\n",
        "    \n",
        "    # Reindex and interpolate\n",
        "    full_predictions = sparse_predictions.reindex(full_timeline)\n",
        "    full_predictions = full_predictions.interpolate(method='linear')\n",
        "    \n",
        "    # Fill any edge NaNs\n",
        "    full_predictions = full_predictions.ffill().bfill()\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"  Prediction range: {full_predictions.min():.3f}m to {full_predictions.max():.3f}m\")\n",
        "    \n",
        "    return full_predictions\n",
        "\n",
        "\n",
        "def predict_ensemble(model, scaler, historical_df, rainfall_ensemble_df,\n",
        "                     X_train_columns, sequence_length=72, horizons=None,\n",
        "                     station_names=None, n_members=20, verbose=True):\n",
        "    \"\"\"\n",
        "    Generate ensemble river flow predictions from multiple rainfall scenarios.\n",
        "    \n",
        "    This is the main prediction function that:\n",
        "    1. Extracts each rainfall ensemble member\n",
        "    2. Runs prediction for each member (FAST - one prediction per member)\n",
        "    3. Returns all predictions for statistical analysis\n",
        "    \n",
        "    Args:\n",
        "        model: Trained PyTorch model\n",
        "        scaler: Fitted scaler\n",
        "        historical_df: Historical data with differential and rainfall\n",
        "        rainfall_ensemble_df: DataFrame with all ensemble members\n",
        "                              Columns like: Osney_member_0, Osney_member_1, ...\n",
        "        X_train_columns: Column names used in training\n",
        "        sequence_length: LSTM sequence length\n",
        "        horizons: List of prediction horizons\n",
        "        station_names: List of station names\n",
        "        n_members: Number of ensemble members to use\n",
        "        verbose: Whether to print progress\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: Each column is one ensemble member's 240-hour prediction\n",
        "    \"\"\"\n",
        "    if horizons is None:\n",
        "        horizons = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24,\n",
        "                    30, 36, 42, 48,\n",
        "                    72, 96, 120, 144, 168, 192, 216, 240]\n",
        "    \n",
        "    if station_names is None:\n",
        "        # Try to infer from column names\n",
        "        station_names = list(set(col.rsplit('_member_', 1)[0] \n",
        "                                  for col in rainfall_ensemble_df.columns \n",
        "                                  if '_member_' in col))\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"ENSEMBLE PREDICTION\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Using {n_members} ensemble members\")\n",
        "    print(f\"Stations: {len(station_names)}\")\n",
        "    print(f\"Forecast horizons: {len(horizons)} points\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    ensemble_predictions = {}\n",
        "    \n",
        "    for member_idx in tqdm(range(n_members), desc=\"Processing ensemble members\"):\n",
        "        # Extract this member's rainfall across all stations\n",
        "        member_columns = [f'{station}_member_{member_idx}' for station in station_names]\n",
        "        \n",
        "        # Check columns exist\n",
        "        missing = [col for col in member_columns if col not in rainfall_ensemble_df.columns]\n",
        "        if missing:\n",
        "            if verbose:\n",
        "                print(f\"  Skipping member {member_idx}: missing {len(missing)} columns\")\n",
        "            continue\n",
        "        \n",
        "        # Create rainfall df for this member (rename columns to station names)\n",
        "        member_rainfall = rainfall_ensemble_df[member_columns].copy()\n",
        "        member_rainfall.columns = station_names\n",
        "        \n",
        "        try:\n",
        "            # Generate prediction for this ensemble member\n",
        "            prediction = predict_single_ensemble_member(\n",
        "                model=model,\n",
        "                scaler=scaler,\n",
        "                historical_df=historical_df,\n",
        "                rainfall_forecast_df=member_rainfall,\n",
        "                X_train_columns=X_train_columns,\n",
        "                sequence_length=sequence_length,\n",
        "                horizons=horizons,\n",
        "                verbose=False\n",
        "            )\n",
        "            \n",
        "            ensemble_predictions[f'member_{member_idx}'] = prediction\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  Error on member {member_idx}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Combine all predictions into a DataFrame\n",
        "    ensemble_df = pd.DataFrame(ensemble_predictions)\n",
        "    \n",
        "    print(f\"\\nâœ“ Generated {len(ensemble_predictions)} ensemble predictions\")\n",
        "    print(f\"  Forecast shape: {ensemble_df.shape}\")\n",
        "    print(f\"  Time range: {ensemble_df.index[0]} to {ensemble_df.index[-1]}\")\n",
        "    \n",
        "    return ensemble_df\n",
        "\n",
        "\n",
        "print(\"âœ“ Prediction functions defined (FIXED VERSION)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch rainfall forecast from Open-Meteo API\n",
        "def get_rainfall_forecast(locations):\n",
        "    \"\"\"\n",
        "    Fetches 10-day hourly rainfall forecast from the Open-Meteo API for multiple locations.\n",
        "    \"\"\"\n",
        "    location_names = list(locations.keys())\n",
        "    latitudes = [loc['latitude'] for loc in locations.values()]\n",
        "    longitudes = [loc['longitude'] for loc in locations.values()]\n",
        "\n",
        "    url = \"https://api.open-meteo.com/v1/forecast\"\n",
        "    params = {\n",
        "        \"latitude\": latitudes,\n",
        "        \"longitude\": longitudes,\n",
        "        \"hourly\": \"precipitation\",\n",
        "        \"forecast_days\": 10\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        forecast_dfs = []\n",
        "        for i, location_data in enumerate(data):\n",
        "            df = pd.DataFrame(location_data['hourly'])\n",
        "            df['timestamp'] = pd.to_datetime(df['time'])\n",
        "            df = df.set_index('timestamp')\n",
        "            df = df[['precipitation']]\n",
        "            df = df.rename(columns={'precipitation': location_names[i]})\n",
        "            forecast_dfs.append(df)\n",
        "        \n",
        "        combined_df = pd.concat(forecast_dfs, axis=1)\n",
        "        print(\"Successfully fetched and processed rainfall forecast.\")\n",
        "        return combined_df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"API request failed: {e}\")\n",
        "        return pd.DataFrame()\n",
        "    except (KeyError, TypeError) as e:\n",
        "        print(f\"Failed to parse API response: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def get_rainfall_forecast_ensemble(locations, ensemble_method='mean', ensemble_model='ecmwf_ifs025'):\n",
        "    \"\"\"\n",
        "    Fetches ensemble rainfall forecast from the Open-Meteo API for multiple locations.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    locations : dict\n",
        "        Dictionary of location names and their coordinates\n",
        "    ensemble_method : str\n",
        "        'mean', 'median', 'percentiles', or 'all'\n",
        "    ensemble_model : str\n",
        "        'ecmwf_ifs025', 'icon_seamless', or 'gfs_seamless'\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        Rainfall forecast data with timestamps as index\n",
        "    \"\"\"\n",
        "    location_names = list(locations.keys())\n",
        "    latitudes = [loc['latitude'] for loc in locations.values()]\n",
        "    longitudes = [loc['longitude'] for loc in locations.values()]\n",
        "\n",
        "    url = \"https://ensemble-api.open-meteo.com/v1/ensemble\"\n",
        "    params = {\n",
        "        \"latitude\": latitudes,\n",
        "        \"longitude\": longitudes,\n",
        "        \"hourly\": \"precipitation\",\n",
        "        \"models\": ensemble_model\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        forecast_dfs = []\n",
        "        for i, location_data in enumerate(data):\n",
        "            df = pd.DataFrame(location_data['hourly'])\n",
        "            df['timestamp'] = pd.to_datetime(df['time'])\n",
        "            df = df.set_index('timestamp')\n",
        "            \n",
        "            # Handle ensemble members\n",
        "            precip_cols = [col for col in df.columns if col.startswith('precipitation')]\n",
        "            \n",
        "            if ensemble_method == 'mean':\n",
        "                df_agg = df[precip_cols].mean(axis=1).to_frame(name=location_names[i])\n",
        "            elif ensemble_method == 'median':\n",
        "                df_agg = df[precip_cols].median(axis=1).to_frame(name=location_names[i])\n",
        "            elif ensemble_method == 'percentiles':\n",
        "                df_agg = pd.DataFrame({\n",
        "                    f\"{location_names[i]}_p10\": df[precip_cols].quantile(0.1, axis=1),\n",
        "                    f\"{location_names[i]}_p50\": df[precip_cols].quantile(0.5, axis=1),\n",
        "                    f\"{location_names[i]}_p90\": df[precip_cols].quantile(0.9, axis=1)\n",
        "                })\n",
        "            elif ensemble_method == 'all':\n",
        "                df_agg = df[precip_cols].copy()\n",
        "                df_agg.columns = [f\"{location_names[i]}_member{j}\" for j in range(len(precip_cols))]\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown ensemble_method: {ensemble_method}\")\n",
        "            \n",
        "            forecast_dfs.append(df_agg)\n",
        "        \n",
        "        combined_df = pd.concat(forecast_dfs, axis=1)\n",
        "        print(f\"Successfully fetched and processed ensemble rainfall forecast ({ensemble_method}).\")\n",
        "        return combined_df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"API request failed: {e}\")\n",
        "        return pd.DataFrame()\n",
        "    except (KeyError, TypeError) as e:\n",
        "        print(f\"Failed to parse API response: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Coordinates for rainfall stations\n",
        "station_coordinates = {\n",
        "    'Osney': {'latitude': 51.750, 'longitude': -1.272},\n",
        "    'Eynsham': {'latitude': 51.789, 'longitude': -1.402},\n",
        "    'St': {'latitude': 51.7, 'longitude': -1.5},\n",
        "    'Shorncote': {'latitude': 51.666, 'longitude': -1.916},\n",
        "    'Rapsgate': {'latitude': 51.815, 'longitude': -1.975},\n",
        "    'Stowell': {'latitude': 51.833, 'longitude': -1.821},\n",
        "    'Bourton': {'latitude': 51.884, 'longitude': -1.758},\n",
        "    'Chipping': {'latitude': 51.942, 'longitude': -1.547},\n",
        "    'Grimsbury': {'latitude': 52.065, 'longitude': -1.326},\n",
        "    'Bicester': {'latitude': 51.899, 'longitude': -1.155},\n",
        "    'Byfield': {'latitude': 52.179, 'longitude': -1.274},\n",
        "    'Swindon': {'latitude': 51.556, 'longitude': -1.779},\n",
        "    'Worsham': {'latitude': 51.817, 'longitude': -1.498}\n",
        "}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ ENSEMBLE OF FORECASTS\n",
        "\n",
        "**The Ultimate Upgrade:** Instead of averaging rainfall ensemble members BEFORE prediction, we now:\n",
        "\n",
        "1. **Get ALL rainfall ensemble members** (50 members from ECMWF)\n",
        "2. **Generate a separate forecast for EACH rainfall member** (50 flag forecasts!)\n",
        "3. **Analyze the ensemble spread** to quantify prediction uncertainty\n",
        "\n",
        "### Why This is Better:\n",
        "- **Preserves full uncertainty chain**: Rainfall uncertainty â†’ Flow uncertainty\n",
        "- **Probabilistic predictions**: Calculate probability of different flag colors\n",
        "- **Distribution-aware**: See the full range of possible outcomes, not just mean/median\n",
        "- **Operational decision support**: \"30% chance of red flag\" is more useful than a single prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_rainfall_forecast_ensemble(locations, ensemble_method='mean', \n",
        "                                   ensemble_model='icon_seamless'):\n",
        "    \"\"\"\n",
        "    Fetches 10-day hourly ENSEMBLE rainfall forecast from the Open-Meteo Ensemble API.\n",
        "    \n",
        "    This function retrieves multiple ensemble members (perturbed forecasts) and combines\n",
        "    them according to the specified method.\n",
        "\n",
        "    Args:\n",
        "        locations (dict): A dictionary where keys are location names and values are dicts\n",
        "                          with 'latitude' and 'longitude'.\n",
        "        ensemble_method (str): How to combine ensemble members:\n",
        "                               - 'mean': Average across all members (default)\n",
        "                               - 'median': Median across all members\n",
        "                               - 'all': Return all individual members\n",
        "                               - 'percentiles': Return 10th, 50th, 90th percentiles\n",
        "        ensemble_model (str): Which ensemble model to use:\n",
        "                               - 'icon_seamless': DWD ICON (default, best for Europe)\n",
        "                               - 'gfs_seamless': NOAA GFS (good for global)\n",
        "                               - 'ecmwf_ifs025': ECMWF (best accuracy, 51 members)\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with a timestamp index and columns for each location's\n",
        "                      predicted rainfall in mm. For ensemble_method='all', columns will be\n",
        "                      named like 'Location_member_0', 'Location_member_1', etc.\n",
        "    \"\"\"\n",
        "    location_names = list(locations.keys())\n",
        "    latitudes = [loc['latitude'] for loc in locations.values()]\n",
        "    longitudes = [loc['longitude'] for loc in locations.values()]\n",
        "\n",
        "    # API endpoint and parameters for ENSEMBLE forecast\n",
        "    # NOTE: Use ensemble-api.open-meteo.com, not api.open-meteo.com\n",
        "    url = \"https://ensemble-api.open-meteo.com/v1/ensemble\"\n",
        "    params = {\n",
        "        \"latitude\": latitudes,\n",
        "        \"longitude\": longitudes,\n",
        "        \"hourly\": \"precipitation\",\n",
        "        \"forecast_days\": 10,\n",
        "        \"models\": ensemble_model\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Make the API request\n",
        "        print(f\"Fetching ensemble forecast from {ensemble_model}...\")\n",
        "        response = requests.get(url, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        # Process the response - ensemble API returns data differently\n",
        "        # For multiple locations, data is a list; for single location, it's a dict\n",
        "        if not isinstance(data, list):\n",
        "            data = [data]  # Convert to list for consistent handling\n",
        "        \n",
        "        forecast_dfs = []\n",
        "        \n",
        "        for i, location_data in enumerate(data):\n",
        "            location_name = location_names[i]\n",
        "            hourly_data = location_data['hourly']\n",
        "            \n",
        "            # Extract timestamps\n",
        "            timestamps = pd.to_datetime(hourly_data['time'])\n",
        "            \n",
        "            # Extract all ensemble members for precipitation\n",
        "            # Ensemble members are named like 'precipitation_member01', 'precipitation_member02', etc.\n",
        "            # Note: they use 2-digit format with leading zeros (01, 02, ... 09, 10, 11, ...)\n",
        "            ensemble_members = []\n",
        "            member_idx = 1  # Start at 1, not 0\n",
        "            \n",
        "            while True:\n",
        "                # Try with leading zero format for members 1-9\n",
        "                member_key = f'precipitation_member{member_idx:02d}'\n",
        "                if member_key in hourly_data:\n",
        "                    member_data = hourly_data[member_key]\n",
        "                    ensemble_members.append(member_data)\n",
        "                    member_idx += 1\n",
        "                else:\n",
        "                    break  # No more members found\n",
        "            \n",
        "            # Convert to DataFrame (rows=time, columns=members)\n",
        "            ensemble_df = pd.DataFrame(ensemble_members).T\n",
        "            ensemble_df.index = timestamps\n",
        "            \n",
        "            print(f\"  {location_name}: Retrieved {len(ensemble_members)} ensemble members\")\n",
        "            \n",
        "            # Combine ensemble members according to specified method\n",
        "            if ensemble_method == 'mean':\n",
        "                # Take the mean across all ensemble members\n",
        "                result_df = pd.DataFrame({\n",
        "                    location_name: ensemble_df.mean(axis=1)\n",
        "                })\n",
        "            elif ensemble_method == 'median':\n",
        "                # Take the median across all ensemble members\n",
        "                result_df = pd.DataFrame({\n",
        "                    location_name: ensemble_df.median(axis=1)\n",
        "                })\n",
        "            elif ensemble_method == 'percentiles':\n",
        "                # Return 10th, 50th (median), and 90th percentiles\n",
        "                result_df = pd.DataFrame({\n",
        "                    f'{location_name}_p10': ensemble_df.quantile(0.10, axis=1),\n",
        "                    f'{location_name}_p50': ensemble_df.quantile(0.50, axis=1),\n",
        "                    f'{location_name}_p90': ensemble_df.quantile(0.90, axis=1)\n",
        "                })\n",
        "            elif ensemble_method == 'all':\n",
        "                # Return all individual ensemble members\n",
        "                result_df = ensemble_df.copy()\n",
        "                result_df.columns = [f'{location_name}_member_{j}' for j in range(len(ensemble_members))]\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown ensemble_method: {ensemble_method}\")\n",
        "            \n",
        "            forecast_dfs.append(result_df)\n",
        "        \n",
        "        # Combine all location forecasts into a single DataFrame\n",
        "        combined_df = pd.concat(forecast_dfs, axis=1)\n",
        "        \n",
        "        print(f\"\\nâœ“ Successfully fetched and processed ENSEMBLE rainfall forecast\")\n",
        "        print(f\"  Model: {ensemble_model}\")\n",
        "        print(f\"  Method: {ensemble_method}\")\n",
        "        print(f\"  Shape: {combined_df.shape}\")\n",
        "        \n",
        "        return combined_df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"API request failed: {e}\")\n",
        "        return pd.DataFrame()\n",
        "    except (KeyError, TypeError) as e:\n",
        "        print(f\"Failed to parse API response: {e}\")\n",
        "        print(f\"Response structure: {data.keys() if isinstance(data, dict) else 'Not a dict'}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "print(\"âœ“ Ensemble rainfall forecast function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Fetch ALL individual rainfall ensemble members\n",
        "print(\"=\"*80)\n",
        "print(\"STEP 1: Fetching ALL Rainfall Ensemble Members\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "rainfall_forecast_all_members = get_rainfall_forecast_ensemble(\n",
        "    station_coordinates, \n",
        "    ensemble_method='all',  # Get ALL individual members!\n",
        "    ensemble_model='ecmwf_ifs025'  # ECMWF has 50 members\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ Retrieved rainfall ensemble with shape: {rainfall_forecast_all_members.shape}\")\n",
        "print(f\"  Columns: {len(rainfall_forecast_all_members.columns)}\")\n",
        "print(f\"  Hours: {len(rainfall_forecast_all_members)}\")\n",
        "\n",
        "# Count how many members per station\n",
        "station_names = list(station_coordinates.keys())\n",
        "members_per_station = len([col for col in rainfall_forecast_all_members.columns if col.startswith(f'{station_names[0]}_member_')])\n",
        "print(f\"  Ensemble members per station: {members_per_station}\")\n",
        "print(f\"  Total stations: {len(station_names)}\")\n",
        "\n",
        "print(\"\\nSample column names:\")\n",
        "print(rainfall_forecast_all_members.columns[:5].tolist())\n",
        "print(\"...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Generate ensemble predictions (FAST - one prediction per member)\n",
        "# This uses the new efficient prediction function\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2: Generating Ensemble River Flow Predictions\")\n",
        "print(\"=\"*80)\n",
        "print(\"Each ensemble member has different rainfall â†’ different river prediction\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Generate predictions for ALL 50 ensemble members\n",
        "# This should take ~30-60 seconds total\n",
        "n_members_to_use = 50\n",
        "\n",
        "ensemble_predictions_df = predict_ensemble(\n",
        "    model=best_model,\n",
        "    scaler=scaler,\n",
        "    historical_df=isis_df_featureless,\n",
        "    rainfall_ensemble_df=rainfall_forecast_all_members,\n",
        "    X_train_columns=X_isis.columns,\n",
        "    sequence_length=sequence_length,\n",
        "    horizons=horizons,\n",
        "    station_names=station_names,\n",
        "    n_members=n_members_to_use,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Show sample of the predictions\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ENSEMBLE PREDICTIONS SAMPLE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nFirst few hours:\")\n",
        "print(ensemble_predictions_df.head())\n",
        "print(f\"\\nLast few hours:\")\n",
        "print(ensemble_predictions_df.tail())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Calculate ensemble statistics from the 20 timeseries\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 3: Computing Ensemble Statistics\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# The new ensemble_predictions_df has:\n",
        "# - Index: hourly timestamps (241 rows: hour 0 to hour 240)\n",
        "# - Columns: member_0, member_1, ..., member_19\n",
        "\n",
        "# Calculate statistics across all ensemble members\n",
        "ensemble_stats = pd.DataFrame({\n",
        "    'mean': ensemble_predictions_df.mean(axis=1),\n",
        "    'median': ensemble_predictions_df.median(axis=1),\n",
        "    'std': ensemble_predictions_df.std(axis=1),\n",
        "    'p05': ensemble_predictions_df.quantile(0.05, axis=1),\n",
        "    'p10': ensemble_predictions_df.quantile(0.10, axis=1),\n",
        "    'p25': ensemble_predictions_df.quantile(0.25, axis=1),\n",
        "    'p75': ensemble_predictions_df.quantile(0.75, axis=1),\n",
        "    'p90': ensemble_predictions_df.quantile(0.90, axis=1),\n",
        "    'p95': ensemble_predictions_df.quantile(0.95, axis=1),\n",
        "    'min': ensemble_predictions_df.min(axis=1),\n",
        "    'max': ensemble_predictions_df.max(axis=1),\n",
        "})\n",
        "\n",
        "print(f\"âœ“ Statistics calculated for {len(ensemble_predictions_df)} hourly timesteps\")\n",
        "print(f\"  Statistics available: {list(ensemble_stats.columns)}\")\n",
        "print(f\"  Number of ensemble members: {len(ensemble_predictions_df.columns)}\")\n",
        "\n",
        "# Show spread at key horizons\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ENSEMBLE SPREAD AT KEY HORIZONS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Horizon':<12} {'Mean':>10} {'Std Dev':>10} {'IQR':>10} {'Range':>12}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for hours_ahead in [0, 6, 12, 24, 48, 72, 120, 168, 240]:\n",
        "    if hours_ahead < len(ensemble_stats):\n",
        "        row = ensemble_stats.iloc[hours_ahead]\n",
        "        iqr = row['p75'] - row['p25']\n",
        "        rng = row['max'] - row['min']\n",
        "        print(f\"{hours_ahead}h ahead    {row['mean']:>10.3f}m {row['std']:>10.4f}m {iqr:>10.4f}m {rng:>10.4f}m\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check if ensemble members are actually different\n",
        "print(\"\\nâœ“ Verification: Are ensemble members different?\")\n",
        "member_means = ensemble_predictions_df.mean(axis=0)\n",
        "print(f\"  Mean prediction by member (should vary if working correctly):\")\n",
        "print(f\"  {member_means.values[:5].round(4)} ... {member_means.values[-3:].round(4)}\")\n",
        "print(f\"  Range across members: {member_means.max() - member_means.min():.4f}m\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Visualize the 20 Ensemble Predictions (all 240 hours)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 4: Visualizing Ensemble Predictions\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Configuration: Choose threshold set\n",
        "# Options: 'fixed' (0.215, 0.33, 0.44, 0.535) or 'historical' (0.1366, 0.2582, 0.387, 0.6047)\n",
        "THRESHOLD_SET = 'historical'  # Change to 'historical' to use old thresholds\n",
        "\n",
        "# Define both threshold sets\n",
        "FIXED_THRESHOLDS = {\n",
        "    'green': (-float('inf'), 0.215),\n",
        "    'light_blue': (0.215, 0.33),\n",
        "    'dark_blue': (0.33, 0.44),\n",
        "    'amber': (0.44, 0.535),\n",
        "    'red': (0.535, float('inf'))\n",
        "}\n",
        "\n",
        "HISTORICAL_THRESHOLDS = {\n",
        "    'green': (-float('inf'), 0.1366),\n",
        "    'light_blue': (0.1366, 0.2582),\n",
        "    'dark_blue': (0.2582, 0.387),\n",
        "    'amber': (0.387, 0.6047),\n",
        "    'red': (0.6047, float('inf'))\n",
        "}\n",
        "\n",
        "# Select the threshold set based on configuration\n",
        "if THRESHOLD_SET == 'fixed':\n",
        "    FLAG_THRESHOLDS = FIXED_THRESHOLDS\n",
        "    print(f\"Using FIXED thresholds: {[FIXED_THRESHOLDS['light_blue'][0], FIXED_THRESHOLDS['dark_blue'][0], FIXED_THRESHOLDS['amber'][0], FIXED_THRESHOLDS['red'][0]]}\")\n",
        "elif THRESHOLD_SET == 'historical':\n",
        "    FLAG_THRESHOLDS = HISTORICAL_THRESHOLDS\n",
        "    print(f\"Using HISTORICAL thresholds: {[HISTORICAL_THRESHOLDS['light_blue'][0], HISTORICAL_THRESHOLDS['dark_blue'][0], HISTORICAL_THRESHOLDS['amber'][0], HISTORICAL_THRESHOLDS['red'][0]]}\")\n",
        "else:\n",
        "    raise ValueError(f\"THRESHOLD_SET must be 'fixed' or 'historical', got '{THRESHOLD_SET}'\")\n",
        "\n",
        "FLAG_COLORS = {\n",
        "    'green': '#008001',\n",
        "    'light_blue': '#02bfff',\n",
        "    'dark_blue': '#000080',\n",
        "    'amber': '#ffa503',\n",
        "    'red': '#ff0000'\n",
        "}\n",
        "\n",
        "# Create a comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 1, figsize=(18, 12), height_ratios=[2, 1])\n",
        "\n",
        "# ============= Plot 1: Fan Chart with all ensemble members =============\n",
        "ax1 = axes[0]\n",
        "\n",
        "# Make index timezone-naive for cleaner plotting\n",
        "plot_df = ensemble_predictions_df.copy()\n",
        "if hasattr(plot_df.index, 'tz') and plot_df.index.tz is not None:\n",
        "    plot_df.index = plot_df.index.tz_localize(None)\n",
        "    \n",
        "plot_stats = ensemble_stats.copy()\n",
        "if hasattr(plot_stats.index, 'tz') and plot_stats.index.tz is not None:\n",
        "    plot_stats.index = plot_stats.index.tz_localize(None)\n",
        "\n",
        "# Add flag color background bands using FLAG_THRESHOLDS\n",
        "ax1.axhspan(-2, FLAG_THRESHOLDS['light_blue'][0], color=FLAG_COLORS['green'], alpha=0.15, zorder=0, label='Green Flag')\n",
        "ax1.axhspan(FLAG_THRESHOLDS['light_blue'][0], FLAG_THRESHOLDS['dark_blue'][0], color=FLAG_COLORS['light_blue'], alpha=0.15, zorder=0, label='Light Blue')\n",
        "ax1.axhspan(FLAG_THRESHOLDS['dark_blue'][0], FLAG_THRESHOLDS['amber'][0], color=FLAG_COLORS['dark_blue'], alpha=0.15, zorder=0, label='Dark Blue')\n",
        "ax1.axhspan(FLAG_THRESHOLDS['amber'][0], FLAG_THRESHOLDS['red'][0], color=FLAG_COLORS['amber'], alpha=0.15, zorder=0, label='Amber')\n",
        "ax1.axhspan(FLAG_THRESHOLDS['red'][0], 2, color=FLAG_COLORS['red'], alpha=0.15, zorder=0, label='Red Flag')\n",
        "\n",
        "# Plot individual ensemble members (thin lines)\n",
        "for col in plot_df.columns:\n",
        "    ax1.plot(plot_df.index, plot_df[col], color='steelblue', alpha=0.3, linewidth=0.8, zorder=1)\n",
        "\n",
        "# Plot uncertainty bands (filled regions)\n",
        "ax1.fill_between(plot_stats.index, plot_stats['p05'], plot_stats['p95'], \n",
        "                  alpha=0.2, color='blue', label='5th-95th percentile', zorder=2)\n",
        "ax1.fill_between(plot_stats.index, plot_stats['p25'], plot_stats['p75'], \n",
        "                  alpha=0.3, color='blue', label='25th-75th percentile (IQR)', zorder=3)\n",
        "\n",
        "# Plot ensemble mean\n",
        "ax1.plot(plot_stats.index, plot_stats['mean'], color='darkred', linewidth=2.5, \n",
        "         label='Ensemble Mean', zorder=4)\n",
        "\n",
        "# Formatting\n",
        "ax1.set_title(f'10-Day River Differential Forecast with Ensemble Uncertainty\\n({n_members_to_use} rainfall scenarios)', \n",
        "              fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Date/Time', fontsize=12)\n",
        "ax1.set_ylabel('River Differential (m)', fontsize=12)\n",
        "ax1.legend(loc='upper right', fontsize=9)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Set reasonable y limits\n",
        "y_min = min(plot_stats['min'].min(), -0.1)\n",
        "y_max = max(plot_stats['max'].max(), 0.7)\n",
        "ax1.set_ylim(y_min - 0.05, y_max + 0.05)\n",
        "\n",
        "# Add vertical lines for day markers\n",
        "for i in range(1, 11):\n",
        "    day_time = plot_df.index[0] + pd.Timedelta(hours=24*i)\n",
        "    if day_time <= plot_df.index[-1]:\n",
        "        ax1.axvline(x=day_time, color='gray', linestyle='--', alpha=0.3)\n",
        "        ax1.text(day_time, y_max, f'Day {i}', ha='center', va='bottom', fontsize=9, alpha=0.5)\n",
        "\n",
        "# ============= Plot 2: Ensemble Spread Over Time =============\n",
        "ax2 = axes[1]\n",
        "\n",
        "# Plot standard deviation and IQR over time\n",
        "ax2.fill_between(plot_stats.index, 0, plot_stats['std'], alpha=0.5, color='coral', label='Std Dev')\n",
        "ax2.plot(plot_stats.index, plot_stats['p75'] - plot_stats['p25'], color='darkgreen', \n",
        "         linewidth=2, label='IQR (P75-P25)')\n",
        "ax2.plot(plot_stats.index, plot_stats['max'] - plot_stats['min'], color='purple', \n",
        "         linewidth=1.5, linestyle='--', label='Full Range (Max-Min)')\n",
        "\n",
        "ax2.set_title('Forecast Uncertainty Over Time', fontsize=12, fontweight='bold')\n",
        "ax2.set_xlabel('Date/Time', fontsize=12)\n",
        "ax2.set_ylabel('Spread (m)', fontsize=12)\n",
        "ax2.legend(loc='upper right', fontsize=9)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_ylim(0, None)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('ensemble_forecast.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ“ Plot saved as 'ensemble_forecast.png'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Calculate FLAG PROBABILITIES from ensemble\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 5: Calculating Flag Probabilities Over Time\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Use the same thresholds as FLAG_THRESHOLDS for consistency\n",
        "# FLAG_BOUNDARIES will use the same threshold set selected in cell 34 (THRESHOLD_SET)\n",
        "FLAG_BOUNDARIES = FLAG_THRESHOLDS.copy()\n",
        "\n",
        "def calculate_flag_probabilities_new(ensemble_df, flag_boundaries):\n",
        "    \"\"\"\n",
        "    Calculate the probability of each flag color at each timestep.\n",
        "    \n",
        "    Args:\n",
        "        ensemble_df: DataFrame where columns are ensemble members, rows are hours\n",
        "        flag_boundaries: Dict of flag name -> (lower, upper) thresholds\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with probability columns for each flag at each timestep\n",
        "    \"\"\"\n",
        "    n_members = ensemble_df.shape[1]\n",
        "    probs_df = pd.DataFrame(index=ensemble_df.index)\n",
        "    \n",
        "    for flag_name, (lower, upper) in flag_boundaries.items():\n",
        "        # Count how many members predict each flag at each timestep\n",
        "        if upper == float('inf'):\n",
        "            count = (ensemble_df >= lower).sum(axis=1)\n",
        "        elif lower == -float('inf'):\n",
        "            count = (ensemble_df < upper).sum(axis=1)\n",
        "        else:\n",
        "            count = ((ensemble_df >= lower) & (ensemble_df < upper)).sum(axis=1)\n",
        "        \n",
        "        probs_df[flag_name] = count / n_members * 100  # Convert to percentage\n",
        "    \n",
        "    return probs_df\n",
        "\n",
        "# Calculate flag probabilities for all 241 timesteps\n",
        "flag_probabilities = calculate_flag_probabilities_new(\n",
        "    ensemble_predictions_df, \n",
        "    FLAG_BOUNDARIES\n",
        ")\n",
        "\n",
        "print(f\"âœ“ Flag probabilities calculated for {len(flag_probabilities)} timesteps\")\n",
        "print(f\"  Flags tracked: {list(flag_probabilities.columns)}\")\n",
        "\n",
        "# Show flag probabilities at key horizons\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FLAG PROBABILITIES AT KEY HORIZONS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Hour':<8} {'Green':>8} {'Lt Blue':>10} {'Dk Blue':>10} {'Amber':>8} {'Red':>8}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for hours_ahead in [0, 6, 12, 24, 48, 72, 120, 168, 240]:\n",
        "    if hours_ahead < len(flag_probabilities):\n",
        "        row = flag_probabilities.iloc[hours_ahead]\n",
        "        print(f\"{hours_ahead:<8} {row['green']:>7.1f}% {row['light_blue']:>9.1f}% \"\n",
        "              f\"{row['dark_blue']:>9.1f}% {row['amber']:>7.1f}% {row['red']:>7.1f}%\")\n",
        "\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Visualize Flag Probabilities Over Time\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 6: Flag Probability Visualization\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create stacked area chart of flag probabilities over time\n",
        "fig, ax = plt.subplots(figsize=(18, 6))\n",
        "\n",
        "# Make index timezone-naive for plotting\n",
        "plot_probs = flag_probabilities.copy()\n",
        "if hasattr(plot_probs.index, 'tz') and plot_probs.index.tz is not None:\n",
        "    plot_probs.index = plot_probs.index.tz_localize(None)\n",
        "\n",
        "# Order flags from low to high risk\n",
        "flag_order = ['green', 'light_blue', 'dark_blue', 'amber', 'red']\n",
        "colors = ['#008001', '#02bfff', '#000080', '#ffa503', '#ff0000']\n",
        "\n",
        "# Create stacked area plot\n",
        "ax.stackplot(plot_probs.index, \n",
        "             [plot_probs[flag] for flag in flag_order],\n",
        "             labels=['Green', 'Light Blue', 'Dark Blue', 'Amber', 'Red'],\n",
        "             colors=colors,\n",
        "             alpha=0.8)\n",
        "\n",
        "# Formatting\n",
        "ax.set_title(f'Flag Probability Distribution Over 10-Day Forecast\\n(based on {n_members_to_use} rainfall ensemble members)', \n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('Date/Time', fontsize=12)\n",
        "ax.set_ylabel('Probability (%)', fontsize=12)\n",
        "ax.set_ylim(0, 100)\n",
        "ax.legend(loc='upper right', fontsize=10)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add day markers\n",
        "for i in range(1, 11):\n",
        "    day_time = plot_probs.index[0] + pd.Timedelta(hours=24*i)\n",
        "    if day_time <= plot_probs.index[-1]:\n",
        "        ax.axvline(x=day_time, color='white', linestyle='-', linewidth=1, alpha=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('flag_probabilities.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ“ Plot saved as 'flag_probabilities.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: All Ensemble Members Spaghetti Plot with Rainfall\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 7: Spaghetti Plot - All Ensemble Members with Rainfall\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20, 12))\n",
        "ax_rain = ax.twinx()  # Secondary axis for rainfall\n",
        "\n",
        "# Define flag colors\n",
        "flag_colors = {\n",
        "    'green': '#008001',\n",
        "    'light_blue': '#02bfff',\n",
        "    'dark_blue': '#000080',\n",
        "    'amber': '#ffa503',\n",
        "    'red': '#ff0000'\n",
        "}\n",
        "\n",
        "# Add flag boundaries as horizontal filled regions using FLAG_BOUNDARIES\n",
        "# (FLAG_BOUNDARIES is defined in cell 35 and uses the same threshold set as FLAG_THRESHOLDS)\n",
        "ax.axhspan(-4, FLAG_BOUNDARIES['light_blue'][0], color=flag_colors['green'], alpha=0.08, zorder=0)\n",
        "ax.axhspan(FLAG_BOUNDARIES['light_blue'][0], FLAG_BOUNDARIES['dark_blue'][0], color=flag_colors['light_blue'], alpha=0.08, zorder=0)\n",
        "ax.axhspan(FLAG_BOUNDARIES['dark_blue'][0], FLAG_BOUNDARIES['amber'][0], color=flag_colors['dark_blue'], alpha=0.08, zorder=0)\n",
        "ax.axhspan(FLAG_BOUNDARIES['amber'][0], FLAG_BOUNDARIES['red'][0], color=flag_colors['amber'], alpha=0.08, zorder=0)\n",
        "ax.axhspan(FLAG_BOUNDARIES['red'][0], 4, color=flag_colors['red'], alpha=0.08, zorder=0)\n",
        "\n",
        "# ============= RAINFALL DATA PREPARATION =============\n",
        "print(\"Preparing rainfall data...\")\n",
        "\n",
        "# 1. Historical rainfall (last 7 days) - aggregate across all stations\n",
        "last_7_days = isis_df_featureless.iloc[-24*7:].copy()\n",
        "if hasattr(last_7_days.index, 'tz') and last_7_days.index.tz is not None:\n",
        "    last_7_days.index = last_7_days.index.tz_localize(None)\n",
        "\n",
        "rainfall_cols = [col for col in last_7_days.columns if col != 'differential']\n",
        "historical_rainfall_hourly = last_7_days[rainfall_cols].mean(axis=1)  # Average across stations\n",
        "historical_rainfall_daily = historical_rainfall_hourly.resample('1D').sum()  # Daily totals\n",
        "\n",
        "# 2. Forecast rainfall - calculate ensemble statistics\n",
        "rainfall_forecast_naive = rainfall_forecast_all_members.copy()\n",
        "if hasattr(rainfall_forecast_naive.index, 'tz') and rainfall_forecast_naive.index.tz is not None:\n",
        "    rainfall_forecast_naive.index = rainfall_forecast_naive.index.tz_localize(None)\n",
        "\n",
        "# For each timestamp, sum across all stations for each member, then get statistics\n",
        "n_members_rain = 50  # All ECMWF ensemble members\n",
        "station_names_list = list(station_coordinates.keys())\n",
        "\n",
        "# Calculate average rainfall per ensemble member (mean across all stations)\n",
        "member_totals = pd.DataFrame(index=rainfall_forecast_naive.index)\n",
        "for member_idx in range(n_members_rain):\n",
        "    member_cols = [f'{station}_member_{member_idx}' for station in station_names_list]\n",
        "    existing_cols = [col for col in member_cols if col in rainfall_forecast_naive.columns]\n",
        "    if existing_cols:\n",
        "        member_totals[f'member_{member_idx}'] = rainfall_forecast_naive[existing_cols].mean(axis=1)\n",
        "\n",
        "# Resample to daily for cleaner visualization\n",
        "member_totals_daily = member_totals.resample('1D').sum()\n",
        "\n",
        "# Calculate rainfall ensemble statistics\n",
        "forecast_rain_median = member_totals_daily.median(axis=1)\n",
        "forecast_rain_p10 = member_totals_daily.quantile(0.10, axis=1)\n",
        "forecast_rain_p90 = member_totals_daily.quantile(0.90, axis=1)\n",
        "\n",
        "# Error bars: distance from median to percentiles\n",
        "error_lower = forecast_rain_median - forecast_rain_p10\n",
        "error_upper = forecast_rain_p90 - forecast_rain_median\n",
        "\n",
        "print(f\"  Historical rainfall: {len(historical_rainfall_daily)} daily bars (avg across stations)\")\n",
        "print(f\"  Forecast rainfall: {len(forecast_rain_median)} daily bars with ensemble spread (P10-P90)\")\n",
        "\n",
        "# ============= PLOT RAINFALL BARS =============\n",
        "bar_width = 0.8  # Width in days (for daily bars)\n",
        "\n",
        "# Historical rainfall bars (gray)\n",
        "ax_rain.bar(historical_rainfall_daily.index, historical_rainfall_daily.values,\n",
        "           width=bar_width, color='gray', alpha=0.4, label='Historical Rainfall', zorder=1)\n",
        "\n",
        "# Forecast rainfall bars with error bars showing ensemble spread\n",
        "ax_rain.bar(forecast_rain_median.index, forecast_rain_median.values,\n",
        "           width=bar_width, color='cornflowerblue', alpha=0.5,\n",
        "           yerr=[error_lower.values, error_upper.values],\n",
        "           error_kw={'elinewidth': 1.5, 'capsize': 3, 'capthick': 1, 'alpha': 0.7, 'color': 'navy'},\n",
        "           label='Forecast Rainfall (median Â± P10-P90)', zorder=2)\n",
        "\n",
        "# ============= PLOT RIVER DIFFERENTIAL =============\n",
        "# Plot historical differential as a solid black line\n",
        "ax.plot(last_7_days.index, last_7_days['differential'].values, \n",
        "        color='black', linewidth=3, label='Historical Differential', zorder=100, alpha=0.9)\n",
        "\n",
        "# Current time marker\n",
        "current_time = last_7_days.index[-1]\n",
        "ax.axvline(x=current_time, color='red', linestyle='--', linewidth=2.5, \n",
        "          alpha=0.8, label='Now', zorder=101)\n",
        "\n",
        "# Make ensemble predictions timezone-naive for plotting\n",
        "plot_ensemble = ensemble_predictions_df.copy()\n",
        "if hasattr(plot_ensemble.index, 'tz') and plot_ensemble.index.tz is not None:\n",
        "    plot_ensemble.index = plot_ensemble.index.tz_localize(None)\n",
        "\n",
        "# Plot ALL ensemble member river forecasts (spaghetti plot)\n",
        "n_members = len(plot_ensemble.columns)\n",
        "print(f\"Plotting {n_members} ensemble member trajectories...\")\n",
        "\n",
        "for idx, col in enumerate(plot_ensemble.columns):\n",
        "    if idx == 0:\n",
        "        ax.plot(plot_ensemble.index, plot_ensemble[col].values, \n",
        "                color='steelblue', linewidth=1.2, alpha=0.5, \n",
        "                label=f'Ensemble Predictions (n={n_members})', zorder=50)\n",
        "    else:\n",
        "        ax.plot(plot_ensemble.index, plot_ensemble[col].values, \n",
        "                color='steelblue', linewidth=1.2, alpha=0.5, zorder=50)\n",
        "\n",
        "# Overlay the ensemble MEAN as a bold line\n",
        "ax.plot(plot_ensemble.index, ensemble_stats['mean'].values, \n",
        "       color='darkviolet', linewidth=3, \n",
        "       label='Ensemble Mean', zorder=102, alpha=0.95)\n",
        "\n",
        "# Also plot the ensemble MEDIAN\n",
        "ax.plot(plot_ensemble.index, ensemble_stats['median'].values, \n",
        "       color='darkgreen', linewidth=2.5, linestyle='--',\n",
        "       label='Ensemble Median', zorder=103, alpha=0.85)\n",
        "\n",
        "# ============= FORMATTING =============\n",
        "ax.set_xlabel('Date/Time', fontsize=14, fontweight='bold')\n",
        "ax.set_ylabel('River Differential (m)', fontsize=14, fontweight='bold', color='black')\n",
        "ax_rain.set_ylabel('Rainfall (mm/day, avg across stations)', fontsize=14, fontweight='bold', color='cornflowerblue')\n",
        "\n",
        "ax.tick_params(axis='y', labelcolor='black')\n",
        "ax_rain.tick_params(axis='y', labelcolor='cornflowerblue')\n",
        "\n",
        "# Set y-limits\n",
        "ax.set_ylim(-0.1, max(0.9, ensemble_stats['max'].max() + 0.1))\n",
        "max_rain = max(historical_rainfall_daily.max(), forecast_rain_p90.max()) if len(forecast_rain_p90) > 0 else historical_rainfall_daily.max()\n",
        "ax_rain.set_ylim(0, max_rain * 1.3)\n",
        "\n",
        "ax.set_title(f'10-Day River Forecast - All {n_members} Ensemble Members\\n'\n",
        "             f'(Different rainfall scenarios produce different river predictions)', \n",
        "            fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "ax.grid(True, alpha=0.3, linestyle=':', linewidth=0.8)\n",
        "\n",
        "# Combined legend\n",
        "lines1, labels1 = ax.get_legend_handles_labels()\n",
        "lines2, labels2 = ax_rain.get_legend_handles_labels()\n",
        "ax.legend(lines1 + lines2, labels1 + labels2, fontsize=10, loc='upper left', framealpha=0.9)\n",
        "\n",
        "# Add day markers\n",
        "for i in range(1, 11):\n",
        "    day_time = current_time + pd.Timedelta(days=i)\n",
        "    if day_time <= plot_ensemble.index[-1]:\n",
        "        ax.axvline(x=day_time, color='gray', linestyle=':', alpha=0.3, zorder=0)\n",
        "\n",
        "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.savefig('spaghetti_with_rainfall.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ“ Spaghetti plot with rainfall complete!\")\n",
        "print(f\"  River: {n_members} ensemble forecasts (blue lines) + mean (purple) + median (green dashed)\")\n",
        "print(f\"  Rainfall: Historical (gray bars) + Forecast with P10-P90 error bars (blue bars)\")\n",
        "print(f\"  Saved as 'spaghetti_with_rainfall.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 8: Total Rainfall vs Final Differential (Scatter Plot)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 8: Total Rainfall vs Final River Differential\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calculate total rainfall for each ensemble member over the 10-day forecast\n",
        "rainfall_forecast_naive = rainfall_forecast_all_members.copy()\n",
        "station_names_list = list(station_coordinates.keys())\n",
        "n_members = 50  # All ECMWF ensemble members\n",
        "\n",
        "total_rainfall_by_member = []\n",
        "final_differential_by_member = []\n",
        "member_labels = []\n",
        "\n",
        "for member_idx in range(n_members):\n",
        "    # Get total rainfall for this member (sum across all stations and all hours)\n",
        "    member_cols = [f'{station}_member_{member_idx}' for station in station_names_list]\n",
        "    existing_cols = [col for col in member_cols if col in rainfall_forecast_naive.columns]\n",
        "    \n",
        "    if existing_cols:\n",
        "        total_rain = rainfall_forecast_naive[existing_cols].sum().sum()  # Sum across stations and time\n",
        "        total_rainfall_by_member.append(total_rain)\n",
        "        \n",
        "        # Get final differential for this member (at hour 240)\n",
        "        final_diff = ensemble_predictions_df[f'member_{member_idx}'].iloc[-1]\n",
        "        final_differential_by_member.append(final_diff)\n",
        "        \n",
        "        member_labels.append(member_idx)\n",
        "\n",
        "# Create scatter plot\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Plot each point\n",
        "scatter = ax.scatter(total_rainfall_by_member, final_differential_by_member, \n",
        "                     c=member_labels, cmap='viridis', s=150, alpha=0.8, edgecolors='black', linewidths=1)\n",
        "\n",
        "# Add member labels to each point\n",
        "for i, (x, y, label) in enumerate(zip(total_rainfall_by_member, final_differential_by_member, member_labels)):\n",
        "    ax.annotate(f'{label}', (x, y), textcoords=\"offset points\", xytext=(5, 5), \n",
        "                fontsize=9, alpha=0.8)\n",
        "\n",
        "# Add trend line\n",
        "z = np.polyfit(total_rainfall_by_member, final_differential_by_member, 1)\n",
        "p = np.poly1d(z)\n",
        "x_line = np.linspace(min(total_rainfall_by_member), max(total_rainfall_by_member), 100)\n",
        "ax.plot(x_line, p(x_line), 'r--', linewidth=2, alpha=0.7, label=f'Trend line')\n",
        "\n",
        "# Calculate correlation\n",
        "correlation = np.corrcoef(total_rainfall_by_member, final_differential_by_member)[0, 1]\n",
        "\n",
        "# Add flag threshold lines using FLAG_BOUNDARIES\n",
        "ax.axhline(y=FLAG_BOUNDARIES['light_blue'][0], color='green', linestyle=':', alpha=0.5, label='Green/Light Blue threshold')\n",
        "ax.axhline(y=FLAG_BOUNDARIES['dark_blue'][0], color='cyan', linestyle=':', alpha=0.5, label='Light Blue/Dark Blue threshold')\n",
        "ax.axhline(y=FLAG_BOUNDARIES['amber'][0], color='blue', linestyle=':', alpha=0.5, label='Dark Blue/Amber threshold')\n",
        "ax.axhline(y=FLAG_BOUNDARIES['red'][0], color='orange', linestyle=':', alpha=0.5, label='Amber/Red threshold')\n",
        "\n",
        "# Formatting\n",
        "ax.set_xlabel('Total 10-Day Rainfall (mm, all stations)', fontsize=14, fontweight='bold')\n",
        "ax.set_ylabel('Final River Differential at Day 10 (m)', fontsize=14, fontweight='bold')\n",
        "ax.set_title(f'Rainfall vs River Response: How Different Rainfall Totals Affect Final River Level\\n'\n",
        "             f'Correlation: r = {correlation:.3f}', fontsize=14, fontweight='bold')\n",
        "\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.legend(loc='upper left', fontsize=9)\n",
        "\n",
        "# Add colorbar for member index\n",
        "cbar = plt.colorbar(scatter, ax=ax, label='Ensemble Member')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('rainfall_vs_differential.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(f\"\\nâœ“ Scatter plot complete!\")\n",
        "print(f\"  Correlation coefficient: r = {correlation:.3f}\")\n",
        "print(f\"\\nSummary by ensemble member:\")\n",
        "print(f\"{'Member':<8} {'Total Rain (mm)':<18} {'Final Diff (m)':<15} {'Flag':<12}\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "for member, rain, diff in sorted(zip(member_labels, total_rainfall_by_member, final_differential_by_member), key=lambda x: x[1]):\n",
        "    if diff >= FLAG_BOUNDARIES['red'][0]:\n",
        "        flag = 'RED'\n",
        "    elif diff >= FLAG_BOUNDARIES['amber'][0]:\n",
        "        flag = 'AMBER'\n",
        "    elif diff >= FLAG_BOUNDARIES['dark_blue'][0]:\n",
        "        flag = 'DARK BLUE'\n",
        "    elif diff >= FLAG_BOUNDARIES['light_blue'][0]:\n",
        "        flag = 'LIGHT BLUE'\n",
        "    else:\n",
        "        flag = 'GREEN'\n",
        "    print(f\"{member:<8} {rain:<18.1f} {diff:<15.3f} {flag:<12}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: Historical what-if forecast using ACTUAL future rainfall on 2020-01-20\n",
        "# \n",
        "# This cell answers: \"What would the model have predicted on 2020-01-20 if it had\n",
        "# perfect knowledge of the future rainfall?\"\n",
        "# It compares that prediction with what actually happened from Jan 10â€“30, 2020.\n",
        "\n",
        "# 1. Define forecast time (t0)\n",
        "forecast_time = pd.Timestamp('2020-01-10 00:00:00')\n",
        "if hasattr(isis_df_featureless.index, 'tz') and isis_df_featureless.index.tz is not None:\n",
        "    forecast_time = forecast_time.tz_localize(isis_df_featureless.index.tz)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 7: Historical what-if forecast on 2020-01-20 using ACTUAL rainfall\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Forecast time (t0): {forecast_time}\")\n",
        "\n",
        "# 2. Build historical data up to t0\n",
        "historical_until_t0 = isis_df_featureless.loc[:forecast_time].copy()\n",
        "\n",
        "# Require at least 30 days of history for features and sequence_length hours for LSTM\n",
        "min_history_hours = max(720, sequence_length + 1)\n",
        "if len(historical_until_t0) < min_history_hours:\n",
        "    raise ValueError(f\"Not enough history before {forecast_time}. \"\n",
        "                     f\"Need {min_history_hours} hours, got {len(historical_until_t0)}.\")\n",
        "\n",
        "# 3. Build a \"forecast\" rainfall DataFrame using ACTUAL future rainfall from Jan 20â€“30, 2020\n",
        "start_future = forecast_time\n",
        "end_future = forecast_time + pd.Timedelta(hours=240)  # 10 days\n",
        "\n",
        "rainfall_station_cols = [c for c in isis_df_featureless.columns if c != 'differential']\n",
        "actual_future_rainfall = isis_df_featureless.loc[start_future:end_future, rainfall_station_cols].copy()\n",
        "\n",
        "if actual_future_rainfall.empty:\n",
        "    raise ValueError(\"Actual future rainfall data for Jan 20â€“30, 2020 is missing in isis_df_featureless.\")\n",
        "\n",
        "print(f\"Using ACTUAL rainfall from {actual_future_rainfall.index.min()} to {actual_future_rainfall.index.max()} for what-if forecast.\")\n",
        "\n",
        "# 4. Run a single-member prediction where the rainfall forecast = actual future rainfall\n",
        "clairvoyant_pred = predict_single_ensemble_member(\n",
        "    model=best_model,\n",
        "    scaler=scaler,\n",
        "    historical_df=historical_until_t0,\n",
        "    rainfall_forecast_df=actual_future_rainfall,\n",
        "    X_train_columns=X_isis.columns,\n",
        "    sequence_length=sequence_length,\n",
        "    horizons=horizons,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# 5. Extract actual differential for comparison over Jan 10â€“30, 2020\n",
        "window_start = pd.Timestamp('2020-01-10 00:00:00')\n",
        "window_end = pd.Timestamp('2020-01-30 23:00:00')\n",
        "if hasattr(isis_df_featureless.index, 'tz') and isis_df_featureless.index.tz is not None:\n",
        "    window_start = window_start.tz_localize(isis_df_featureless.index.tz)\n",
        "    window_end = window_end.tz_localize(isis_df_featureless.index.tz)\n",
        "\n",
        "actual_diff_window = isis_df_featureless['differential'].loc[window_start:window_end]\n",
        "\n",
        "# Align clairvoyant prediction to the same plotting window (predictions start at t0)\n",
        "clairvoyant_window = clairvoyant_pred.loc[forecast_time:window_end]\n",
        "\n",
        "# 6. Plot comparison: 10 days before and 10 days after t0\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Plot actual history\n",
        "plt.plot(actual_diff_window.index, actual_diff_window.values,\n",
        "         label='Actual differential', color='black', linewidth=2)\n",
        "\n",
        "# Plot clairvoyant prediction (only from t0 onwards)\n",
        "plt.plot(clairvoyant_window.index, clairvoyant_window.values,\n",
        "         label='Model prediction (with actual future rainfall)',\n",
        "         color='tab:blue', linewidth=2)\n",
        "\n",
        "# Mark forecast time\n",
        "ymin, ymax = plt.ylim()\n",
        "plt.axvline(forecast_time, color='red', linestyle='--', linewidth=1.5,\n",
        "            label='Forecast time (2020-01-20 00:00)')\n",
        "\n",
        "plt.title('What-if forecast on 2020-01-20 with actual rainfall\\nJan 10â€“30, 2020')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('River differential (m)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 8: Historical what-if forecasts for EACH day in a specified month\n",
        "# \n",
        "# For each day in the specified month, this cell asks:\n",
        "# \"What would the model have predicted at 00Z on this day if it had perfect\n",
        "# knowledge of the future rainfall?\"\n",
        "#\n",
        "# We:\n",
        "# 1. Use historical data up to t0 (the day's 00Z)\n",
        "# 2. Use ACTUAL future rainfall from t0 onward as the \"forecast\"\n",
        "# 3. Generate a 10-day clairvoyant prediction for each t0\n",
        "# 4. Plot all predictions and the actual differential on a single plot\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 8: Historical what-if forecasts for EACH day in specified month (00Z)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Configuration - CHOOSE YOUR MONTH AND YEAR HERE\n",
        "year = 2024\n",
        "month = 10\n",
        "\n",
        "# Determine number of days in the month\n",
        "import calendar\n",
        "num_days = calendar.monthrange(year, month)[1]\n",
        "month_days = range(1, num_days + 1)\n",
        "\n",
        "# Timezone handling\n",
        "if hasattr(isis_df_featureless.index, 'tz') and isis_df_featureless.index.tz is not None:\n",
        "    tz = isis_df_featureless.index.tz\n",
        "else:\n",
        "    tz = None\n",
        "\n",
        "# History requirement: 30 days of features + LSTM sequence length\n",
        "min_history_hours = max(720, sequence_length + 1)\n",
        "\n",
        "# Rainfall station columns (all non-differential columns)\n",
        "rainfall_station_cols = [c for c in isis_df_featureless.columns if c != 'differential']\n",
        "\n",
        "clairvoyant_preds = {}\n",
        "skipped = []\n",
        "\n",
        "for d in month_days:\n",
        "    # 1) Define t0 for this day\n",
        "    t0 = pd.Timestamp(f\"{year}-{month:02d}-{d:02d} 00:00:00\")\n",
        "    if tz is not None:\n",
        "        t0 = t0.tz_localize(tz)\n",
        "\n",
        "    # Make sure we actually have a data point at t0\n",
        "    if t0 not in isis_df_featureless.index:\n",
        "        skipped.append((t0, \"no data at t0\"))\n",
        "        continue\n",
        "\n",
        "    # 2) Historical data up to t0\n",
        "    hist_until_t0 = isis_df_featureless.loc[:t0].copy()\n",
        "    if len(hist_until_t0) < min_history_hours:\n",
        "        skipped.append((t0, f\"not enough history ({len(hist_until_t0)} < {min_history_hours})\"))\n",
        "        continue\n",
        "\n",
        "    # 3) Build ACTUAL future rainfall from t0 to t0+240h (10 days)\n",
        "    start_future = t0\n",
        "    end_future = t0 + pd.Timedelta(hours=240)\n",
        "    actual_future_rainfall = isis_df_featureless.loc[start_future:end_future, rainfall_station_cols].copy()\n",
        "    if actual_future_rainfall.empty:\n",
        "        skipped.append((t0, \"no future rainfall data\"))\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n--- Running what-if forecast for {t0} ---\")\n",
        "\n",
        "    # 4) Run what-if prediction for this t0\n",
        "    pred = predict_single_ensemble_member(\n",
        "        model=best_model,\n",
        "        scaler=scaler,\n",
        "        historical_df=hist_until_t0,\n",
        "        rainfall_forecast_df=actual_future_rainfall,\n",
        "        X_train_columns=X_isis.columns,\n",
        "        sequence_length=sequence_length,\n",
        "        horizons=horizons,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    clairvoyant_preds[t0] = pred\n",
        "\n",
        "print(f\"\\nâœ“ Generated {len(clairvoyant_preds)} what-if forecasts\")\n",
        "if skipped:\n",
        "    print(\"Skipped t0 values:\")\n",
        "    for t0, reason in skipped:\n",
        "        print(f\"  - {t0}: {reason}\")\n",
        "\n",
        "# Comparison window: full month plus 10 days after last t0\n",
        "window_start = pd.Timestamp(f'{year}-{month:02d}-01 00:00:00')\n",
        "# Calculate end of window (10 days after last day of month)\n",
        "last_day_of_month = pd.Timestamp(f'{year}-{month:02d}-{num_days:02d} 23:00:00')\n",
        "window_end = last_day_of_month + pd.Timedelta(days=10)\n",
        "if tz is not None:\n",
        "    window_start = window_start.tz_localize(tz)\n",
        "    window_end = window_end.tz_localize(tz)\n",
        "\n",
        "actual_diff_window = isis_df_featureless['differential'].loc[window_start:window_end]\n",
        "\n",
        "# Get month name for title\n",
        "month_name = calendar.month_name[month]\n",
        "\n",
        "plt.figure(figsize=(16, 7))\n",
        "\n",
        "# Plot actual differential\n",
        "plt.plot(actual_diff_window.index, actual_diff_window.values,\n",
        "         label='Actual differential', color='black', linewidth=2)\n",
        "\n",
        "# Plot all clairvoyant predictions (one per day)\n",
        "if clairvoyant_preds:\n",
        "    n = len(clairvoyant_preds)\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, n))\n",
        "\n",
        "    for (t0, color) in zip(sorted(clairvoyant_preds.keys()), colors):\n",
        "        series = clairvoyant_preds[t0]\n",
        "        # Restrict to plotting window\n",
        "        series_window = series.loc[window_start:window_end]\n",
        "        plt.plot(series_window.index, series_window.values,\n",
        "                 color=color, alpha=0.5)\n",
        "\n",
        "    # Add a single legend entry for all prediction curves\n",
        "    plt.plot([], [], color='tab:blue', alpha=0.5,\n",
        "             label=f'Model predictions (one per day, clairvoyant rainfall)')\n",
        "\n",
        "# Mark each forecast time with a faint vertical line\n",
        "for t0 in sorted(clairvoyant_preds.keys()):\n",
        "    plt.axvline(t0, color='red', linestyle=':', alpha=0.2)\n",
        "\n",
        "plt.title(f'What-if forecasts using ACTUAL rainfall\\nOne 10-day forecast per day in {month_name} {year} (00Z starts)')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('River differential (m)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv_new",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
